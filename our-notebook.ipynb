{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3hN-efIwnVxD",
   "metadata": {
    "id": "3hN-efIwnVxD"
   },
   "source": [
    "# Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bcb08f",
   "metadata": {},
   "source": [
    "To get started with running on Colab, run the following script after changing CURRENT_DIR to the correct path to the project folder. **Note**: Colab does not fully support interactive Matplotlib plots, so some functions like loss visualisation during training may not work fully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elzySajjkdbz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23039,
     "status": "ok",
     "timestamp": 1635870709074,
     "user": {
      "displayName": "Joel Loo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11818416161416341433"
     },
     "user_tz": -480
    },
    "id": "elzySajjkdbz",
    "outputId": "7e3218e6-7428-4189-9d82-7d3bb2238699"
   },
   "outputs": [],
   "source": [
    "# If running on Colab\n",
    "%matplotlib inline\n",
    "\n",
    "CURRENT_DIR = '/content/drive/My Drive/Classes/CS5242 project'\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bc75cd",
   "metadata": {},
   "source": [
    "If running the notebook locally (e.g. Jupyter or Jupyter Labs), run the following script instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eY3chhRdkjJq",
   "metadata": {
    "id": "eY3chhRdkjJq"
   },
   "outputs": [],
   "source": [
    "# If running locally\n",
    "%matplotlib notebook\n",
    "CURRENT_DIR = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00283ef",
   "metadata": {},
   "source": [
    "Run the following script regardless of whether you are running locally or on Colab, to set up your paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "A9pF3dgaqQNl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 699,
     "status": "ok",
     "timestamp": 1635870709766,
     "user": {
      "displayName": "Joel Loo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11818416161416341433"
     },
     "user_tz": -480
    },
    "id": "A9pF3dgaqQNl",
    "outputId": "c7eed81e-26c2-494e-90e3-270657d747fe"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir(CURRENT_DIR)\n",
    "sys.path.append(CURRENT_DIR)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f313fd07",
   "metadata": {
    "id": "f313fd07"
   },
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaa7faa",
   "metadata": {
    "id": "aeaa7faa"
   },
   "source": [
    "## Environment and robot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36daf550",
   "metadata": {
    "id": "36daf550"
   },
   "source": [
    "Our robot is a shaped as a 2d planar box with size equals to 0.2 meters. It can move simultaneously both horizontally and vertically. Therefore, its configuration space is a 2d rectangle which size is determined by the maze size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8c5689",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1635870709766,
     "user": {
      "displayName": "Joel Loo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11818416161416341433"
     },
     "user_tz": -480
    },
    "id": "0f8c5689"
   },
   "outputs": [],
   "source": [
    "class MyPlanarRobot():\n",
    "    def __init__(self, base_xy_bounds=5.0) -> None:\n",
    "        self.num_dim = 2\n",
    "        self.joint_idx=[0,1]\n",
    "        self.size = 0.2\n",
    "\n",
    "        self.joint_bounds = []\n",
    "        self.joint_bounds.append([-base_xy_bounds, base_xy_bounds]) # x\n",
    "        self.joint_bounds.append([-base_xy_bounds, base_xy_bounds]) # y\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def get_joint_bounds(self):\n",
    "        return self.joint_bounds\n",
    "\n",
    "    def get_joint_lower_bounds(self):\n",
    "        robot_bounds_low = [bound[0] for bound in self.joint_bounds]\n",
    "        return robot_bounds_low\n",
    "\n",
    "    def get_joint_higher_bounds(self):\n",
    "        robot_bounds_high = [bound[1] for bound in self.joint_bounds]\n",
    "        return robot_bounds_high\n",
    "\n",
    "    def get_cur_state(self):\n",
    "        return self.state\n",
    "\n",
    "    def set_state(self, state):\n",
    "        self.state = state\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = [0] * self.num_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83be3dd3",
   "metadata": {
    "id": "83be3dd3"
   },
   "source": [
    "Our environment is a 2d maze with size 5m * 5m. It is filled with random generated square obstacles of fixed size 1m * 1m.  The difficulties of the maze can be manipulated by altering the number of obstacles present. The maze can be visualized by an occupancy grid with resolution 0.5m, making it essentially an image of size 10 x 10. \n",
    "\n",
    "To facilitate path planning, the maze class also contains code to sample valid start and goals of the point robot and perform collision checking for a given robot configuration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dd910d",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1635870709767,
     "user": {
      "displayName": "Joel Loo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11818416161416341433"
     },
     "user_tz": -480
    },
    "id": "c8dd910d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "import sys\n",
    "import os.path as osp\n",
    "\n",
    "# -------------- Settings ----------------\n",
    "RANDOM = True\n",
    "TOTAL_START_GOAL_CNT = 50\n",
    "MAZE_SIZE = 5\n",
    "OCC_GRID_RESOLUTION = 0.1\n",
    "SMALL_OCC_GRID_RESLUTION = 0.5\n",
    "\n",
    "class Maze2D():\n",
    "    def __init__(self):\n",
    "        self.obstacles = []\n",
    "\n",
    "        # load robot\n",
    "        robot = MyPlanarRobot(base_xy_bounds = MAZE_SIZE / 2.0)\n",
    "        self.robot = robot\n",
    "\n",
    "        # 2d occupancy grid\n",
    "        self.occ_grid_size = int(MAZE_SIZE / OCC_GRID_RESOLUTION)\n",
    "        self.occ_grid = np.zeros((self.occ_grid_size, self.occ_grid_size), dtype=np.uint8)\n",
    "        self.small_occ_grid_size = int(MAZE_SIZE / SMALL_OCC_GRID_RESLUTION)\n",
    "\n",
    "        # clear obstacles\n",
    "        self.clear_obstacles()\n",
    "\n",
    "        # add surrounding walls\n",
    "        half_size = MAZE_SIZE / 2.0\n",
    "        # add wall\n",
    "        self.add_box([half_size + 0.1, 0, 1], [0.1, half_size, 1])\n",
    "        self.add_box([-half_size - 0.1, 0, 1], [0.1, half_size, 1])\n",
    "        self.add_box([0, half_size + 0.1, 1], [half_size, 0.1, 1])\n",
    "        self.add_box([0, -half_size - 0.1, 1], [half_size, 0.1, 1])\n",
    "\n",
    "        # internal attributes\n",
    "        self.goal_robot_id = None\n",
    "        self.path = None\n",
    "        self.approx_path = None\n",
    "        self.sg_pairs = None\n",
    "\n",
    "    def clear_obstacles(self):\n",
    "        self.occ_grid.fill(0)\n",
    "        self.obstacle_dict = {}\n",
    "        self.inflated_occ_grid = None\n",
    "\n",
    "    def random_obstacles(self, num_of_boxes = 8):\n",
    "        # add random obstacles with boxes.\n",
    "        # box_positions = [(-2.25, 2.25)]\n",
    "        box_positions = []\n",
    "\n",
    "        for _ in range(num_of_boxes):\n",
    "            x = random.randint(0, 4)\n",
    "            y = random.randint(0, 4)\n",
    "            x = x - 2\n",
    "            y = y - 2\n",
    "            box_positions.append((x, y))\n",
    "\n",
    "        # print(box_positions)\n",
    "        for box_pos in box_positions:\n",
    "            self.add_box([box_pos[0], box_pos[1], 0.5], [0.5, 0.5, 0.5])\n",
    "\n",
    "        self.obstacle_dict[\"box\"] = box_positions\n",
    "        self.get_inflated_occ_grid()\n",
    "\n",
    "    def add_box(self, box_pos, half_box_size):\n",
    "        # for occupancy grid, center is at upper left corner, unit is cm\n",
    "        half_size = MAZE_SIZE / 2.0\n",
    "        tmp = int(1 / OCC_GRID_RESOLUTION)\n",
    "        cx = (-box_pos[1] + half_size) * tmp\n",
    "        cy = (box_pos[0] + half_size) * tmp\n",
    "        x_size = half_box_size[1] * tmp\n",
    "        y_size = half_box_size[0] * tmp\n",
    "        for x in range(max(0, int(cx - x_size)), min(self.occ_grid_size, int(cx + x_size))):\n",
    "            for y in range(max(0, int(cy - y_size)), min(self.occ_grid_size, int(cy + y_size))):\n",
    "                self.occ_grid[x, y] = 1\n",
    "\n",
    "    def get_occupancy_grid(self):\n",
    "        return self.occ_grid\n",
    "\n",
    "    def get_small_occupancy_grid(self):\n",
    "        occ_grid_small = np.zeros((self.small_occ_grid_size, self.small_occ_grid_size), dtype=np.int8)\n",
    "        for i in range(self.small_occ_grid_size):\n",
    "            for j in range(self.small_occ_grid_size):\n",
    "                occ_grid_small[i, j] = (np.max(self.occ_grid[i*5:(i+1)*5, j*5:(j+1)*5]) == 1)\n",
    "        return occ_grid_small\n",
    "\n",
    "    def get_obstacle_dict(self):\n",
    "        return self.obstacle_dict.copy()\n",
    "\n",
    "    def load_obstacle_dict(self, obstacle_dict):\n",
    "        if \"box\" in obstacle_dict:\n",
    "            for box_pos in obstacle_dict[\"box\"]:\n",
    "                self.add_box([box_pos[0], box_pos[1], 0.5], [0.5, 0.5, 0.5])\n",
    "\n",
    "        self.obstacle_dict = obstacle_dict\n",
    "        \n",
    "    @classmethod\n",
    "    def load_small_occupancy_grid(cls, small_occ_grid):\n",
    "        maze = cls()\n",
    "        maze.occ_grid = np.zeros((maze.occ_grid_size, maze.occ_grid_size), dtype=np.uint8)\n",
    "        if len(small_occ_grid) != maze.small_occ_grid_size * maze.small_occ_grid_size:\n",
    "            raise RuntimeError(\"Input occupancy grid does not match hardcoded maze size!\")\n",
    "        \n",
    "        small_occ_grid = np.array(small_occ_grid).astype(np.int8).reshape(maze.small_occ_grid_size, -1)\n",
    "        for i in range(maze.small_occ_grid_size):\n",
    "            for j in range(maze.small_occ_grid_size):\n",
    "                if small_occ_grid[i, j] == 1:                    \n",
    "                    maze.occ_grid[i*5:(i+1)*5, j*5:(j+1)*5] = np.ones((5, 5), dtype=np.uint8)\n",
    "        return maze\n",
    "\n",
    "    def sample_start_goal(self):\n",
    "        while True:\n",
    "            start = [0] * self.robot.num_dim\n",
    "            goal = [0] * self.robot.num_dim\n",
    "            low_bounds = self.robot.get_joint_lower_bounds()\n",
    "            high_bounds = self.robot.get_joint_higher_bounds()\n",
    "            for i in range(self.robot.num_dim):\n",
    "                start[i] = random.uniform(low_bounds[i], high_bounds[i])\n",
    "                goal[i] = random.uniform(low_bounds[i], high_bounds[i])\n",
    "\n",
    "            if self.is_state_valid(start) and self.is_state_valid(goal):\n",
    "                self.start = start\n",
    "                self.goal = goal\n",
    "                break\n",
    "\n",
    "        print(\"Maze2d: start: {}\".format(self.start))\n",
    "        print(\"Maze2d: goal: {}\".format(self.goal))\n",
    "\n",
    "    def get_inflated_occ_grid(self):\n",
    "        if self.inflated_occ_grid is None:\n",
    "            tmp = np.zeros((self.occ_grid_size + 2, self.occ_grid_size + 2), dtype=np.uint8)\n",
    "            tmp[:self.occ_grid_size, :self.occ_grid_size] += self.occ_grid\n",
    "            tmp[1:self.occ_grid_size + 1, :self.occ_grid_size] += self.occ_grid\n",
    "            tmp[2:, :self.occ_grid_size] += self.occ_grid\n",
    "            tmp[:self.occ_grid_size, 1:self.occ_grid_size+1] += self.occ_grid\n",
    "            tmp[1:self.occ_grid_size + 1, 1:self.occ_grid_size+1] += self.occ_grid\n",
    "            tmp[2:, 1:self.occ_grid_size+1] += self.occ_grid\n",
    "            tmp[:self.occ_grid_size, 2:] += self.occ_grid\n",
    "            tmp[1:self.occ_grid_size + 1, 2:] += self.occ_grid\n",
    "            tmp[2:, 2:] += self.occ_grid\n",
    "            tmp[tmp > 0] = 1\n",
    "\n",
    "            self.inflated_occ_grid = tmp[1:self.occ_grid_size + 1, 1:self.occ_grid_size + 1]\n",
    "\n",
    "    def is_state_valid(self, robot_state):\n",
    "        # Inflate obstacle for collision checking\n",
    "        self.get_inflated_occ_grid()\n",
    "\n",
    "        y, x = robot_state[0], robot_state[1]\n",
    "        x = int((MAZE_SIZE / 2.0 - x) / 0.1)\n",
    "        y = int((y + MAZE_SIZE / 2.0) / 0.1)\n",
    "    \n",
    "        if 0 <= x and x < self.inflated_occ_grid.shape[0] and 0 <= y and y < self.inflated_occ_grid.shape[1]:\n",
    "            return (self.inflated_occ_grid[x, y] != 1)\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bb55b4",
   "metadata": {
    "id": "e7bb55b4"
   },
   "source": [
    "Let's generate a random environment and sample a random start and goal configuration of the robot and visualize the problem. The start configuration is shown in yellow and the goal configuration is shown in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274477a0",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1635870711191,
     "user": {
      "displayName": "Joel Loo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11818416161416341433"
     },
     "user_tz": -480
    },
    "id": "274477a0"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def visualize_data(occ_g, start_pos, goal_pos, path, predicted_path=None):\n",
    "    occ_g = np.array(occ_g).reshape(10, 10)\n",
    "    fig, ax = plt.subplots(1, figsize=(10, 10), dpi=100)\n",
    "    occ_grid_size = occ_g.shape[0]\n",
    "    tmp = occ_grid_size / 4.0 - 0.25\n",
    "    s = (10.0 / occ_grid_size * 100 / 2) ** 2 + 500\n",
    "    for i in range(occ_grid_size):\n",
    "        for j in range(occ_grid_size):\n",
    "            if occ_g[i,j] == 1:\n",
    "                plt.scatter(j/2.0 - tmp, tmp - i/2.0, color=\"black\", marker='s', s=s, alpha=1) # init\n",
    "\n",
    "    ax.add_patch(patches.Rectangle((start_pos[0]-0.1, start_pos[1]-0.1), 0.2, 0.2, facecolor='y'))\n",
    "    ax.add_patch(patches.Rectangle((goal_pos[0]-0.1, goal_pos[1]-0.1), 0.2, 0.2, facecolor='r'))\n",
    "    for i, next_pos in enumerate(path):\n",
    "        ax.text(next_pos[0]+0.06, next_pos[1]+0.06, str(i), {'color': 'g', 'size': 'large'})\n",
    "        ax.add_patch(patches.Rectangle((next_pos[0]-0.07, next_pos[1]-0.07), 0.14, 0.14, facecolor='g'))\n",
    "    if predicted_path is not None:\n",
    "        for i, predicted_pos in enumerate(predicted_path):\n",
    "            ax.text(predicted_pos[0] - 0.11, predicted_pos[1] - 0.11, str(i), {'color': 'b', 'size': 'large'})\n",
    "            ax.add_patch(patches.Rectangle((predicted_pos[0]-0.05, predicted_pos[1]-0.05), 0.1, 0.1, facecolor='b'))\n",
    "\n",
    "    ax.set_title(\"Visualization\")\n",
    "    ax.set_xlim(-2.5,2.5)\n",
    "    ax.set_ylim(-2.5,2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caeb304",
   "metadata": {
    "id": "2caeb304"
   },
   "outputs": [],
   "source": [
    "maze = Maze2D()\n",
    "maze.random_obstacles()\n",
    "maze.sample_start_goal()\n",
    "\n",
    "occ_grid = maze.get_small_occupancy_grid()\n",
    "visualize_data(occ_grid, maze.start, maze.goal, []) # Just visualise the environment for now, without any expert paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6442a3",
   "metadata": {
    "id": "eb6442a3"
   },
   "source": [
    "## Generate Data from Expert Path Planner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0fc1bb",
   "metadata": {
    "id": "1d0fc1bb"
   },
   "source": [
    "To train a neural path planner, we need to generate a database of path planned by an expert path planner. There are numerous choices of path planners we can use. In this particular case, since our robot state is continuous, we choose to use the classic PRM motion planner. It firstly samples valid configurations of robot uniformly in the whole space and attempts to connect those configurations if the path between the states are collision-free. It results in a dense roadmap that captures the connectivity of the space. Finally, a discrete motion planner such as A* is used to find a path between a given start and goal configurations in the space. Another reason that PRM is particularly useful here is that it is multi-query planner, meaning the generated roadmap can be used to solve multiple queries of different start and goal configurations. \n",
    "\n",
    "To generate our path dataset, we sample 200 different mazes with number of obstacles ranging from 5 to 14. In each environment, we sample 500 random configurations and attempt to connect all valid configurations. We save the resultant roadmap as a networkx graph. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6372553b",
   "metadata": {
    "id": "6372553b"
   },
   "source": [
    "### Generate Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d3e466",
   "metadata": {
    "id": "43d3e466"
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import itertools\n",
    "import random\n",
    "import utils\n",
    "\n",
    "\n",
    "def create_environments(root_dir='./dataset', env_num=0, sparse_num=100, dense_num=500):\n",
    "    if env_num == 0:\n",
    "        return\n",
    "    maze = Maze2D()\n",
    "    for i in range(env_num):\n",
    "        # save\n",
    "        directory = osp.join(root_dir, str(i))\n",
    "        if not osp.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "        num_of_boxes = 5 + i // 50\n",
    "\n",
    "        # env\n",
    "        maze.clear_obstacles()\n",
    "        maze.random_obstacles(num_of_boxes=num_of_boxes)\n",
    "        occ_grid = np.array(maze.get_occupancy_grid()).reshape(50, 50)\n",
    "        occ_grid_small = maze.get_small_occupancy_grid()\n",
    "        obstacle_dict = maze.get_obstacle_dict()\n",
    "        maze.sample_start_goal()\n",
    "        maze.robot.set_state(maze.start)\n",
    "\n",
    "        # dense states\n",
    "        states = []\n",
    "        col_status = []\n",
    "        low = maze.robot.get_joint_lower_bounds()\n",
    "        high = maze.robot.get_joint_higher_bounds()\n",
    "        for _ in range(dense_num):\n",
    "            random_state = [0] * maze.robot.num_dim\n",
    "            for i in range(maze.robot.num_dim):\n",
    "                random_state[i] = random.uniform(low[i], high[i])\n",
    "            col_status.append(maze.is_state_valid(random_state)) # mark collision states\n",
    "            states.append(random_state)\n",
    "\n",
    "        dense_G = nx.DiGraph()\n",
    "        dense_G.add_nodes_from([(\"n{}\".format(i), {\"coords\": ','.join(map(str, state)), \"col\": not col_status[i]}) for i, state in enumerate(states)])\n",
    "\n",
    "        # save\n",
    "        # node_pos = np.array(states)\n",
    "        node_pos = np.array([utils.state_to_numpy(dense_G.nodes[node]['coords']) for node in dense_G.nodes()])\n",
    "        utils.visualize_nodes(occ_grid_small, node_pos, None, None, show=False, save=True, file_name=osp.join(directory, \"dense.png\"))\n",
    "        node_pos = np.array([utils.state_to_numpy(dense_G.nodes[node]['coords']) for node in dense_G.nodes() if not dense_G.nodes[node]['col']])\n",
    "        utils.visualize_nodes(occ_grid_small, node_pos, None, None, show=False, save=True, file_name=osp.join(directory, \"dense_free.png\"))\n",
    "\n",
    "        print(\"connecting dense graph\")\n",
    "        nodes = dense_G.nodes()\n",
    "        node_pairs = itertools.combinations(nodes, 2)\n",
    "        # print(list(node_pairs))\n",
    "        for node_pair in node_pairs:\n",
    "            if not dense_G.has_edge(node_pair[0], node_pair[1]):\n",
    "                s1 = dense_G.nodes[node_pair[0]]['coords']\n",
    "                s2 = dense_G.nodes[node_pair[1]]['coords']\n",
    "                if utils.is_edge_free(maze, s1, s2):\n",
    "                    dense_G.add_edge(node_pair[0], node_pair[1])\n",
    "                    dense_G.add_edge(node_pair[1], node_pair[0])\n",
    "        for u,v in dense_G.edges:\n",
    "            dense_G[u][v]['weight'] = utils.calc_weight_states(dense_G.nodes[u]['coords'], dense_G.nodes[v]['coords'])\n",
    "\n",
    "        # save\n",
    "        nx.write_graphml(dense_G, osp.join(directory, \"dense_g.graphml\"))\n",
    "        with open(osp.join(directory, \"occ_grid.txt\"), 'w') as f:\n",
    "            np.savetxt(f, occ_grid_small.reshape(1, -1))\n",
    "        with open(osp.join(directory, \"obstacle_dict.json\"), 'w') as f:\n",
    "            json.dump(obstacle_dict, f)\n",
    "        utils.visualize_nodes(occ_grid_small, [], None, None, show=False, save=True, file_name=osp.join(directory, \"occ_grid.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbec7607",
   "metadata": {
    "id": "cbec7607"
   },
   "outputs": [],
   "source": [
    "create_environments(root_dir='./test_dataset', env_num=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d6af30",
   "metadata": {
    "id": "d5d6af30"
   },
   "source": [
    "### Generate Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c81a30c",
   "metadata": {
    "id": "1c81a30c"
   },
   "source": [
    "We want to collect multiple paths from a single generated environment. Therefore, we first load the saved roadmap graph, and subsequently nvoke Astar path planner to find shortest path between each possible pair of start and goal configurations. If a path is found, we add to our dataset.\n",
    "\n",
    "Here we ignore path that contains only 2 waypoints. Intuitively, a path with two waypoints means that the start configuration and goal configuration of the robot can be connected by a straigh line. As our environment is not very cluttered, the majority of the path in our dataset may constitute such a straight-line path. This imbalance might incur problems in the latter training stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67bf993",
   "metadata": {
    "id": "f67bf993"
   },
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import sys\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import utils\n",
    "\n",
    "import astar\n",
    "\n",
    "def generate_path(root_dir='./dataset', env_num=0, sample_num=500):\n",
    "    if env_num == 0:\n",
    "        return\n",
    "    print(f'Generating path for data in {root_dir}')\n",
    "    maze = Maze2D()\n",
    "\n",
    "    dataset = []\n",
    "    for i in range(env_num):\n",
    "        print(\"generating paths in env {}\".format(i))\n",
    "        maze.clear_obstacles()\n",
    "        \n",
    "        data_dir = osp.join(root_dir, str(i))\n",
    "        with open(osp.join(data_dir, \"obstacle_dict.json\"), 'r') as f:\n",
    "            obstacle_dict = json.load(f)\n",
    "            maze.load_obstacle_dict(obstacle_dict)\n",
    "\n",
    "        dense_G = nx.read_graphml(osp.join(data_dir, \"dense_g.graphml\"))\n",
    "        occ_grid = np.loadtxt(osp.join(data_dir, \"occ_grid.txt\")).tolist()\n",
    "\n",
    "        # sample trajectories\n",
    "        for start_n in dense_G.nodes():\n",
    "            if dense_G.nodes[start_n]['col']:\n",
    "                continue\n",
    "\n",
    "            for goal_n in dense_G.nodes():\n",
    "                if dense_G.nodes[goal_n]['col']:\n",
    "                    continue\n",
    "\n",
    "            goal_pos = utils.state_to_numpy(dense_G.nodes[goal_n]['coords']).tolist()\n",
    "            path_nodes, dis = astar.astar(dense_G, start_n, goal_n, occ_grid, None, None, None)\n",
    "\n",
    "            # sanity check\n",
    "            total_dist = 0\n",
    "            if len(path_nodes) > 2:\n",
    "                for i, node in enumerate(path_nodes):\n",
    "                    if i < len(path_nodes) - 1:\n",
    "                        start_pos = utils.state_to_numpy(dense_G.nodes[node]['coords']).tolist()\n",
    "                        next_pos = utils.state_to_numpy(dense_G.nodes[path_nodes[i + 1]]['coords']).tolist()\n",
    "                        dist = utils.calc_weight_states(start_pos, next_pos)\n",
    "                        total_dist += dist\n",
    "                # print(total_dist, dis)\n",
    "                assert np.allclose(total_dist, dis)\n",
    "\n",
    "            if len(path_nodes) > 2:\n",
    "                path = []\n",
    "                for i, node in enumerate(path_nodes):\n",
    "                    node_pos = utils.state_to_numpy(dense_G.nodes[node]['coords']).tolist()\n",
    "                    path.append(node_pos)\n",
    "\n",
    "                dataset.append([start_pos, goal_pos, occ_grid, path])\n",
    "    \n",
    "    json_path = osp.join(root_dir, 'data_path.json')\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c8acb3",
   "metadata": {
    "id": "a0c8acb3"
   },
   "outputs": [],
   "source": [
    "generate_path(root_dir=os.path.join(CURRENT_DIR, 'test_dataset/'), env_num=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ac7ba7",
   "metadata": {
    "id": "e9ac7ba7"
   },
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35749acd",
   "metadata": {
    "id": "35749acd"
   },
   "source": [
    "Our dataset contains a paths, a sequence of waypoints. However, our MLP and CNN models predicts only the next waypoints given the current robot configuration. Therefore, the dataset needs to be further processed so that a single training data gives the next waypoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6933062",
   "metadata": {
    "id": "d6933062"
   },
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "for root_dir in ['./dataset', './test_dataset']:\n",
    "    read_path = osp.join(root_dir, 'data_path.json')\n",
    "    with open(read_path, 'r') as _file:\n",
    "        data_path = json.load(_file)\n",
    "\n",
    "    dataset_waypoint = []\n",
    "    for data_point in data_path:\n",
    "        start_pos, goal_pos, occ_grid, path = data_point\n",
    "        for i in range(1, len(path)):\n",
    "            prev_pos = path[i - 1]\n",
    "            current_pos = path[i]\n",
    "            dataset_waypoint.append([prev_pos, goal_pos, occ_grid, current_pos])\n",
    "\n",
    "    save_path = osp.join(root_dir, 'data_waypoints.json')\n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(dataset_waypoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c44b554",
   "metadata": {
    "id": "1c44b554"
   },
   "source": [
    "# Visualization of the generated data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e90dc01",
   "metadata": {
    "id": "7e90dc01"
   },
   "source": [
    "Load the waypoint data and randomly visualize a single data point.\n",
    "\n",
    "- Black: obstacles\n",
    "- Red: goal position\n",
    "- Yellow: current position\n",
    "- Green: the next position the robot should take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502e2fc3",
   "metadata": {
    "id": "502e2fc3"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "WAYPOINT_DATA_FILE_PATH_TO_LOAD = os.path.join(CURRENT_DIR, 'dataset/data_waypoints.json')\n",
    "with open(WAYPOINT_DATA_FILE_PATH_TO_LOAD) as _file:\n",
    "    RAW_DATA = json.load(_file)\n",
    "\n",
    "idx = np.random.randint(len(RAW_DATA))\n",
    "current_pos, goal_pos, occ_grid, next_pos = RAW_DATA[idx]\n",
    "visualize_data(occ_grid, current_pos, goal_pos, [next_pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97037a0b",
   "metadata": {
    "id": "97037a0b"
   },
   "source": [
    "The baseline approach for this project is to test out different architectures on the problem of generating a sensible next waypoint given start and end goals. This can be iterated to get closer to the end goal. We also generate complete paths from the start to the end goal to serve as ground truth trajectories to benchmark our generated paths against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28881f30",
   "metadata": {
    "id": "28881f30"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "import os\n",
    "PATH_DATA_FILE_PATH_TO_LOAD = os.path.join(CURRENT_DIR, 'dataset/data_path.json')\n",
    "with open(PATH_DATA_FILE_PATH_TO_LOAD) as _file:\n",
    "    RAW_DATA = json.load(_file)\n",
    "\n",
    "idx = np.random.randint(len(RAW_DATA))\n",
    "current_pos, goal_pos, occ_grid, path = RAW_DATA[idx]\n",
    "visualize_data(occ_grid, current_pos, goal_pos, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a7a066",
   "metadata": {
    "id": "c0a7a066"
   },
   "source": [
    "# Proposed architectures\n",
    "\n",
    "We propose and evaluate several feedforward architectures based on MLPs and CNNs to try and predict a reasonable next waypoint given a start and end goal. We also propose and evaluate several recurrent architectures to generate more temporally consistent paths. We implement these architectures and their dataloaders here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7980230b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25222,
     "status": "ok",
     "timestamp": 1635870744229,
     "user": {
      "displayName": "Joel Loo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11818416161416341433"
     },
     "user_tz": -480
    },
    "id": "7980230b",
    "outputId": "7c067d22-9b93-4367-d969-4701e6c00020"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fbda5a",
   "metadata": {
    "id": "21fbda5a"
   },
   "source": [
    "## Definitions of feedforward networks and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e301933",
   "metadata": {},
   "source": [
    "We implement a vanilla MLP that takes as input the vectorized occupancy grid concatenated in a single vector with the start and end goals. This MLP directly regresses the next waypoint position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee5e1b7",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1635846498845,
     "user": {
      "displayName": "Joel Loo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11818416161416341433"
     },
     "user_tz": -480
    },
    "id": "cee5e1b7"
   },
   "outputs": [],
   "source": [
    "class MLPDataset(Dataset):\n",
    "    \"\"\"A dataset class for the MLP.\n",
    "    \n",
    "    Input: A vector that concat current position, goal position and occupancy grid vector\n",
    "    Output: Next position\n",
    "    \"\"\"\n",
    "    def __init__(self, raw_data, transform=None, target_transform=None, device=\"cpu\"):\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.device = device\n",
    "        self.dataset = raw_data\n",
    "        print(\"dataset size = {}\".format(len(self.dataset)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_pos, goal_pos, occ_grid, next_pos = self.dataset[idx]\n",
    "\n",
    "        dim = len(start_pos)\n",
    "        start_pos = torch.Tensor(start_pos)\n",
    "        goal_pos = torch.Tensor(goal_pos)\n",
    "        occ_grid = torch.Tensor(occ_grid)\n",
    "        next_pos = torch.Tensor(next_pos)\n",
    "\n",
    "        input = torch.cat((start_pos, goal_pos, occ_grid), dim=0).to(self.device)\n",
    "        next_pos = next_pos.to(self.device)\n",
    "\n",
    "        return input, next_pos\n",
    "    \n",
    "    def get_raw_data(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_from_file(cls, file_path, device='cpu'):\n",
    "        print(\"Loading data from {}\".format(file_path))\n",
    "        with open(file_path, 'r') as f:\n",
    "            dataset = json.load(f)\n",
    "        return cls(dataset, device=device)\n",
    "    \n",
    "    @classmethod\n",
    "    def format_input_for_inference(cls, input):\n",
    "        return input.view(1, -1)\n",
    "    \n",
    "    @classmethod\n",
    "    def format_input_for_motion_planning(cls, input, current_pos):\n",
    "        input[:2] = current_pos\n",
    "        return input.view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9227eae9",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1635846498845,
     "user": {
      "displayName": "Joel Loo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11818416161416341433"
     },
     "user_tz": -480
    },
    "id": "9227eae9"
   },
   "outputs": [],
   "source": [
    "class MLPModel(nn.Module):\n",
    "    \"\"\"A trivially simple MLP model.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(104, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.layers(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d491b79",
   "metadata": {},
   "source": [
    "As experiments detailed in the next section show, the MLP alone has difficulty with consistently estimating waypoints in non-obstacle regions. Since the MLP seems to not have fully learned the spatial cues needed to effectively distinguish obstacle/non-obstacle regions, we attempt to design an architecture comprising an MLP-based encoder (which learns to compress occupancy grid information as part of an autoencoder) followed by an MLP head which regresses the waypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e4ff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPAutoencoderDataset(Dataset):\n",
    "    \"\"\"A dataset class for the MLP.\n",
    "    \n",
    "    Input: A vector that concat current position, goal position and occupancy grid vector\n",
    "    Output: Next position\n",
    "    \"\"\"\n",
    "    def __init__(self, raw_data, transform=None, target_transform=None, device=\"cpu\"):\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.device = device\n",
    "        self.dataset = raw_data\n",
    "        print(\"dataset size = {}\".format(len(self.dataset)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        _, _, occ_grid, _ = self.dataset[idx]\n",
    "        occ_grid = torch.Tensor(occ_grid)\n",
    "        occ_grid = occ_grid.to(self.device)\n",
    "        return occ_grid\n",
    "    \n",
    "    def get_raw_data(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_from_file(cls, file_path, device='cpu'):\n",
    "        print(\"Loading data from {}\".format(file_path))\n",
    "        with open(file_path, 'r') as f:\n",
    "            dataset = json.load(f)\n",
    "        return cls(dataset, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caf34a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPEncoderModel(nn.Module):\n",
    "    def __init__(self, feature_size):\n",
    "        super(MLPEncoderModel, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, feature_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.layers(input)\n",
    "    \n",
    "class MLPDecoderModel(nn.Module):\n",
    "    def __init__(self, feature_size):\n",
    "        super(MLPDecoderModel, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(feature_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.layers(input)\n",
    "    \n",
    "class MLPAutoencoderModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPAutoencoderModel, self).__init__()\n",
    "        self.encoder = MLPEncoderModel(16)\n",
    "        self.decoder = MLPDecoderModel(16)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        features = self.encoder(input)\n",
    "        output = self.decoder(features)\n",
    "        return self.sig(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb5a1b7",
   "metadata": {},
   "source": [
    "As a next step from the vanilla MLP, we implement a CNN-based model which we hypothesize could better capture the spatial information in the occupancy grid and estimate better waypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mVAlfsa2fTZt",
   "metadata": {
    "executionInfo": {
     "elapsed": 413,
     "status": "ok",
     "timestamp": 1635871813598,
     "user": {
      "displayName": "Joel Loo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11818416161416341433"
     },
     "user_tz": -480
    },
    "id": "mVAlfsa2fTZt"
   },
   "outputs": [],
   "source": [
    "class MLPCNNDataset(Dataset):\n",
    "    def __init__(self, raw_data, transform=None, target_transform=None, device=\"cpu\"):\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.device = device\n",
    "        self.dataset = raw_data\n",
    "        print(\"dataset size = {}\".format(len(self.dataset)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_pos, goal_pos, occ_grid, next_pos = self.dataset[idx]\n",
    "\n",
    "        dim = len(start_pos)\n",
    "        start_pos = torch.Tensor(start_pos)\n",
    "        goal_pos = torch.Tensor(goal_pos)\n",
    "        occ_grid = torch.Tensor(occ_grid)\n",
    "        next_pos = torch.Tensor(next_pos)\n",
    "\n",
    "        positions = torch.cat((start_pos, goal_pos), dim=0).to(self.device)\n",
    "        occ_grid = torch.unsqueeze(torch.reshape(occ_grid, (10, 10)), dim=0).to(self.device)\n",
    "        next_pos = next_pos.to(self.device)\n",
    "\n",
    "        return (occ_grid, positions), next_pos\n",
    "    \n",
    "    def get_raw_data(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_from_file(cls, file_path, device='cpu'):\n",
    "        print(\"Loading data from {}\".format(file_path))\n",
    "        with open(file_path, 'r') as f:\n",
    "          dataset = json.load(f)\n",
    "        return cls(dataset, device=device)\n",
    "    \n",
    "    @classmethod\n",
    "    def format_input_for_inference(cls, input):\n",
    "        occ_grid, pos = input\n",
    "        return occ_grid.view(1, 1, 10, 10), pos.view(1, -1)\n",
    "    \n",
    "    @classmethod\n",
    "    def format_input_for_motion_planning(cls, input, current_pos):\n",
    "        occ_grid, pos = input\n",
    "        pos[:2] = current_pos\n",
    "        return occ_grid.view(1, 1, 10, 10), pos.view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6jZlJzfDHR",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1635846498847,
     "user": {
      "displayName": "Joel Loo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11818416161416341433"
     },
     "user_tz": -480
    },
    "id": "dd6jZlJzfDHR"
   },
   "outputs": [],
   "source": [
    "class MLPCNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPCNNModel, self).__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(num_features=4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(num_features=8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1,1))\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(16+4, 128),\n",
    "            nn.BatchNorm1d(num_features=128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.BatchNorm1d(num_features=128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        img, positions = inputs\n",
    "        img_features = self.cnn(img).squeeze(dim=3).squeeze(dim=2)\n",
    "        features_positions = torch.cat((img_features, positions), dim=1)\n",
    "        output = self.fc(features_positions)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wteFRwN9ZB3f",
   "metadata": {
    "executionInfo": {
     "elapsed": 284,
     "status": "ok",
     "timestamp": 1635871739510,
     "user": {
      "displayName": "Joel Loo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11818416161416341433"
     },
     "user_tz": -480
    },
    "id": "wteFRwN9ZB3f"
   },
   "outputs": [],
   "source": [
    "class CNN_Encoder(nn.Module):\n",
    "    def __init__(self, output_size, input_size=(1, 10, 10)):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.channel_mult = 16\n",
    "\n",
    "        #convolutions\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,\n",
    "                     out_channels=self.channel_mult*1,\n",
    "                     kernel_size=4,\n",
    "                     stride=2,\n",
    "                     padding=1), # out = (16, 5, 5)\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(self.channel_mult*1, self.channel_mult*2, 3, 2, 1), # out = (32, 3, 3)\n",
    "            nn.BatchNorm2d(self.channel_mult*2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(self.channel_mult*2, self.channel_mult*4, 3, 2, 1), # out = (64, 2, 2)\n",
    "            nn.BatchNorm2d(self.channel_mult*4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(self.channel_mult*4, self.channel_mult*8, 4, 2, 1), # out = (128, 1, 1)\n",
    "            nn.BatchNorm2d(self.channel_mult*8),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Flatten()\n",
    "            # nn.Conv2d(self.channel_mult*8, self.channel_mult*16, 3, 2, 1),\n",
    "            # nn.BatchNorm2d(self.channel_mult*16),\n",
    "            # nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        # self.flat_fts = self.get_flat_fts(self.conv)\n",
    "        self.flat_fts = 128\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(self.flat_fts, output_size),\n",
    "            nn.BatchNorm1d(output_size),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "    def get_flat_fts(self, fts):\n",
    "        tmp = torch.ones(1, *self.input_size)\n",
    "        f = fts(tmp)\n",
    "        return int(np.prod(f.size()[1:]))\n",
    "\n",
    "    def extract_feat(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(-1, self.flat_fts)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(-1, self.flat_fts)\n",
    "        return self.linear(x)\n",
    "\n",
    "class CNN_Decoder(nn.Module):\n",
    "    def __init__(self, embedding_size, input_size=(1, 10, 10)):\n",
    "        super(CNN_Decoder, self).__init__()\n",
    "        self.input_height = input_size[1]\n",
    "        self.input_width = input_size[2]\n",
    "        self.input_dim = embedding_size\n",
    "        self.channel_mult = 16\n",
    "        self.output_channels = 1\n",
    "        self.fc_output_dim = 128 # 512\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, self.fc_output_dim),\n",
    "            nn.BatchNorm1d(self.fc_output_dim),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "        self.deconv = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(self.fc_output_dim, self.channel_mult*4, 4, 2, 1, bias=False), # out =(64, 2, 2)\n",
    "            nn.BatchNorm2d(self.channel_mult*4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(self.channel_mult*4, self.channel_mult*2, 3, 2, 1, bias=False), # out =(32, 3, 3)\n",
    "            nn.BatchNorm2d(self.channel_mult*2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(self.channel_mult*2, self.channel_mult*1, 3, 2, 1, bias=False), # out =(16, 5, 5)\n",
    "            nn.BatchNorm2d(self.channel_mult*1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(self.channel_mult*1, self.output_channels, 4, 2, 1, bias=False), # out =(1, 10, 10)\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, self.fc_output_dim, 1, 1)\n",
    "        x = self.deconv(x)\n",
    "        return x\n",
    "\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, embedding_size=64):\n",
    "        super(AE, self).__init__()\n",
    "        self.encoder = CNN_Encoder(embedding_size)\n",
    "        self.decoder = CNN_Decoder(embedding_size)\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        return self.decode(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h76llAryavoz",
   "metadata": {
    "executionInfo": {
     "elapsed": 306,
     "status": "ok",
     "timestamp": 1635871965865,
     "user": {
      "displayName": "Joel Loo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11818416161416341433"
     },
     "user_tz": -480
    },
    "id": "h76llAryavoz"
   },
   "outputs": [],
   "source": [
    "class MLPCNNModel2(nn.Module):\n",
    "    def __init__(self, pretrained_model_path=None):\n",
    "        super(MLPCNNModel2, self).__init__()\n",
    "\n",
    "        tmp_conv_encoder = CNN_Encoder(16)\n",
    "        if pretrained_model_path:\n",
    "            tmp_conv_encoder.load_state_dict(torch.load(os.path.join(CURRENT_DIR, pretrained_model_path)))\n",
    "\n",
    "        self.conv_encoder = nn.Sequential(\n",
    "            tmp_conv_encoder.conv,\n",
    "            tmp_conv_encoder.linear[0]\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(16+4, 128),\n",
    "            nn.BatchNorm1d(num_features=128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.BatchNorm1d(num_features=128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        img, positions = inputs\n",
    "        img_features = self.conv_encoder(img).view(-1, 16)\n",
    "        features_positions = torch.cat((img_features, positions), dim=1)\n",
    "        output = self.fc(features_positions)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832dc3c0",
   "metadata": {
    "id": "832dc3c0"
   },
   "source": [
    "## Definitions of recurrent networks and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XfdqSWimQnU0",
   "metadata": {
    "id": "XfdqSWimQnU0"
   },
   "outputs": [],
   "source": [
    "class MLPRNNDataset(Dataset):\n",
    "    def __init__(self, raw_data, transform=None, target_transform=None, device=\"cpu\"):\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.device = device\n",
    "        self.dataset = raw_data\n",
    "        print(\"dataset size = {}\".format(len(self.dataset)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_pos, goal_pos, occ_grid, path = self.dataset[idx]\n",
    "        path_length = len(path) - 1\n",
    "        \n",
    "        start_seq = torch.Tensor(path[:-1]).to(self.device)\n",
    "        goal_pos = torch.Tensor(path[-1]).to(self.device)\n",
    "        input_seq = torch.cat((start_seq, goal_pos.repeat(path_length, 1)), dim=1)\n",
    "        label_seq = torch.Tensor(path[1:]).to(self.device)\n",
    "        occ_grid = torch.Tensor(occ_grid).to(self.device)\n",
    "\n",
    "        return (input_seq, occ_grid), label_seq\n",
    "    \n",
    "    def get_raw_data(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_from_file(cls, file_path, device='cpu'):\n",
    "        print(\"Loading data from {}\".format(file_path))\n",
    "        with open(file_path, 'r') as f:\n",
    "            dataset = json.load(f)\n",
    "        return cls(dataset, device=device)\n",
    "    \n",
    "    @classmethod\n",
    "    def format_input_for_inference(cls, input):\n",
    "        input_seq, occ_grid = input\n",
    "        return (\n",
    "            torch.unsqueeze(input_seq, 0),\n",
    "            occ_grid.view(1, -1)\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def format_input_for_motion_planning(cls, input, current_pos):\n",
    "        input_seq, occ_grid = input\n",
    "        goal = input_seq[0, 2:]\n",
    "        concat = torch.cat((current_pos, goal))\n",
    "        return concat.view(1, 1, -1), occ_grid.view(1, -1)\n",
    "\n",
    "import random\n",
    "import copy\n",
    "\n",
    "class MLPRNNDataloader():\n",
    "    def __init__(self, raw_data, batch_size, min_dataset_size=0, shuffle=True, \n",
    "                 drop_last=False, device=\"cpu\", uniform_path_length=None):\n",
    "        print(\"Raw data size: \", len(raw_data))\n",
    "        self.shuffle = shuffle\n",
    "            \n",
    "        if uniform_path_length is not None:\n",
    "            def truncate_fn(datum):\n",
    "                _, goal, occ_grid, path = datum\n",
    "                start = path[uniform_path_length - 1]\n",
    "                return (start, goal, occ_grid, path[:uniform_path_length+1])\n",
    "            \n",
    "            filtered_data = filter(lambda datum: len(datum[3]) - 1 >= uniform_path_length, raw_data)\n",
    "            truncated_data = list(map(truncate_fn, filtered_data))\n",
    "            self.datasets = {uniform_path_length:MLPRNNDataset(truncated_data, device=device)}\n",
    "            self.generators = {uniform_path_length:DataLoader(self.datasets[uniform_path_length], batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)}\n",
    "            self.path_lengths = [uniform_path_length]\n",
    "            print(\"All paths have length \", uniform_path_length)\n",
    "            print(\"Reduced dataset size: \", len(truncated_data))\n",
    "            return\n",
    "        \n",
    "        lengths = map(lambda datum: len(datum[3]) - 1, raw_data)\n",
    "        partitions = {k:list() for k in lengths}\n",
    "        for datum in raw_data:\n",
    "            _, _, _, path = datum\n",
    "            partitions[len(path) - 1].append(datum)\n",
    "\n",
    "        remove = []\n",
    "        for k, data_list in partitions.items():\n",
    "            if len(data_list) < min_dataset_size:\n",
    "                remove.append(k)\n",
    "                print(\"Removing all paths of length \", k, \": only \", len(data_list), \" samples\")\n",
    "            else:\n",
    "                print(\"Adding all paths of length \", k, \": \", len(data_list), \" samples\")\n",
    "        for k in remove:\n",
    "            del partitions[k]\n",
    "\n",
    "        self.datasets = {k:MLPRNNDataset(data_list, device=device) for k, data_list in partitions.items()}\n",
    "        self.generators = {k:DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last) for k, dataset in self.datasets.items()}\n",
    "        self.path_lengths = list(self.generators.keys())\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.tmp_path_lengths = copy.deepcopy(self.path_lengths)\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.tmp_path_lengths)\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if len(self.tmp_path_lengths) > 0:\n",
    "            next_path_length = copy.deepcopy(self.tmp_path_lengths[-1])\n",
    "            next_gen = self.generators[next_path_length]\n",
    "            self.tmp_path_lengths.pop()\n",
    "            return next_gen, next_path_length\n",
    "        else:\n",
    "            raise StopIteration\n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_from_file(cls, file_path, batch_size, train_frac, shuffle=True, drop_last=False, device=\"cpu\", uniform_path_length=None):\n",
    "      print(\"Loading data from {}\".format(file_path))\n",
    "      with open(file_path, 'r') as f:\n",
    "          dataset = json.load(f)\n",
    "\n",
    "      train_size = int(train_frac * len(dataset))\n",
    "      if train_size < len(dataset):\n",
    "          train_dataset = dataset[:train_size]\n",
    "          val_dataset = MLPRNNDataset(dataset[train_size:], device=device)\n",
    "      else:\n",
    "          train_dataset = dataset\n",
    "          val_dataset = MLPRNNDataset([], device=device)\n",
    "\n",
    "      return (\n",
    "          cls(train_dataset, batch_size, shuffle=shuffle, drop_last=drop_last, device=device, uniform_path_length=uniform_path_length), \n",
    "          DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "U4YGj_KLodsM",
   "metadata": {
    "id": "U4YGj_KLodsM"
   },
   "outputs": [],
   "source": [
    "class MLPRNNModel(nn.Module):\n",
    "    def __init__(self, latent_size):\n",
    "        super(MLPRNNModel, self).__init__()\n",
    "\n",
    "        self.map_embedding = nn.Sequential(\n",
    "            nn.Linear(100, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, latent_size)\n",
    "        )\n",
    "\n",
    "        self.goal_embedding =  nn.Sequential(\n",
    "            nn.Linear(4, latent_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(latent_size, latent_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(latent_size, latent_size),\n",
    "        )\n",
    "\n",
    "        self.rnn = nn.RNN(latent_size, latent_size, batch_first=True)\n",
    "\n",
    "        self.mlp_regression = nn.Sequential(\n",
    "            nn.Linear(latent_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "        \n",
    "        self.latent_size = latent_size\n",
    "\n",
    "    def forward(self, inputs, h_init, initialise=False):\n",
    "        input_seq, occ_grid = inputs\n",
    "        batch_size, seq_length, num_dims = input_seq.shape\n",
    "\n",
    "        # h_init not explicitly used in this implementation; instead we overwrite h_init with the map embedding\n",
    "        if initialise:\n",
    "            h = torch.unsqueeze(self.map_embedding(occ_grid), 0) + h_init\n",
    "        else:\n",
    "            h = h_init\n",
    "\n",
    "        # RNN\n",
    "        g_seq = self.goal_embedding(input_seq)\n",
    "        h_seq, h_final = self.rnn(g_seq, h)\n",
    "        regressed_seq = self.mlp_regression(h_seq)\n",
    "\n",
    "        return regressed_seq, h_final\n",
    "    \n",
    "    def init_with_zeros(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.latent_size).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c60ebb",
   "metadata": {
    "id": "c8c60ebb"
   },
   "source": [
    "# Training\n",
    "\n",
    "We implement the training pass, and train and evaluate the architectures proposed in the previous section.\n",
    "\n",
    "## Implementing utilities\n",
    "\n",
    "Before implementing the training passes, we develop some utilities needed during training, to help visualise and evaluate the progress of training, and performance of the trained models.\n",
    "\n",
    "This is a utility to visualise the losses on the training and validation sets during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87df49be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "def init_training_loss_visualiser(num_epochs):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.set_xlim(0, num_epochs + 1)\n",
    "    ax.set_ylim(0, 2)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Training and validation loss')\n",
    "    \n",
    "    legend_lines = [Line2D([0], [0], color='b', lw=3),\n",
    "                    Line2D([0], [0], color='r', lw=3)]\n",
    "    ax.legend(legend_lines, ['Training loss', 'Validation loss'])\n",
    "    return fig, ax\n",
    "\n",
    "def visualise_loss(fig, ax, training_losses, val_losses):\n",
    "    epochs = list(range(len(training_losses)))\n",
    "    ax.plot(epochs, training_losses, 'b')\n",
    "    ax.plot(epochs, val_losses, 'r')\n",
    "    fig.canvas.draw()\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75936a4",
   "metadata": {
    "id": "d75936a4"
   },
   "source": [
    "This evaluates the average loss of the model on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9173314",
   "metadata": {
    "executionInfo": {
     "elapsed": 395,
     "status": "ok",
     "timestamp": 1635871772956,
     "user": {
      "displayName": "Joel Loo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11818416161416341433"
     },
     "user_tz": -480
    },
    "id": "f9173314"
   },
   "outputs": [],
   "source": [
    "def evaluate_test_metrics(test_dataloader, model, criterion, is_recurrent=False):\n",
    "    i = 0\n",
    "    total_loss = 0\n",
    "    test_dataloader = [test_dataloader] if not is_recurrent else test_dataloader\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for entry in test_dataloader:\n",
    "            if is_recurrent:\n",
    "                dataloader, path_length = entry\n",
    "                print(f\"Evaluating data with path length: {path_length}\")\n",
    "            else:\n",
    "                dataloader = entry\n",
    "            \n",
    "            for data in dataloader:\n",
    "                # Get batch of data\n",
    "                inputs, labels = data\n",
    "\n",
    "                # Check if an RNN is being evaluated, and initialise hidden state if so\n",
    "                if is_recurrent:\n",
    "                    batch_size = inputs[0].shape[0]\n",
    "                    h_init = model.init_with_zeros(batch_size)\n",
    "                    network_output, _ = model.forward(inputs, h_init, initialise=True)\n",
    "                else:\n",
    "                    network_output = model.forward(inputs)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = criterion(network_output, labels)\n",
    "                # Print statistics\n",
    "                total_loss += loss.detach().item()\n",
    "                i += 1\n",
    "        print(f'Average loss :{total_loss / i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8164a41e",
   "metadata": {
    "id": "8164a41e"
   },
   "source": [
    "This allows us to visualize the waypoints/paths generated by the model and compare it to ground truth waypoints/paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cf1131",
   "metadata": {
    "executionInfo": {
     "elapsed": 298,
     "status": "ok",
     "timestamp": 1635871782874,
     "user": {
      "displayName": "Joel Loo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11818416161416341433"
     },
     "user_tz": -480
    },
    "id": "f7cf1131"
   },
   "outputs": [],
   "source": [
    "def visualize_model_output(dataset, model, is_recurrent=False):\n",
    "    idx = np.random.randint(len(dataset))\n",
    "    inputs, _ = dataset[idx]\n",
    "    current_pos, goal_pos, occ_grid, gt = dataset.get_raw_data(idx)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if is_recurrent:\n",
    "            h_init = model.init_with_zeros(1)\n",
    "            predicted, _ = model.forward(dataset.format_input_for_inference(inputs), h_init, initialise=True)\n",
    "            predicted_path = torch.squeeze(predicted.detach()).cpu().numpy()\n",
    "            visualize_data(occ_grid, gt[0], gt[-1], gt[1:], predicted_path=predicted_path)\n",
    "        else:\n",
    "            predicted = model.forward(dataset.format_input_for_inference(inputs))\n",
    "            visualize_data(occ_grid, current_pos, goal_pos, [gt], predicted_path=[predicted.detach().cpu().numpy().flatten()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ba7532",
   "metadata": {},
   "source": [
    "## Implementing training pass\n",
    "\n",
    "Implement the training procedures for both feedforward as well as recurrent network architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d72ea0d",
   "metadata": {},
   "source": [
    "We implement a training loop for feedforward architectures designed to regress the waypoints given the occupancy grid, start and end goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2ca8b3",
   "metadata": {
    "executionInfo": {
     "elapsed": 306,
     "status": "ok",
     "timestamp": 1635871777962,
     "user": {
      "displayName": "Joel Loo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11818416161416341433"
     },
     "user_tz": -480
    },
    "id": "8e2ca8b3"
   },
   "outputs": [],
   "source": [
    "def train_feedforward(\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        eval_dataloader,\n",
    "        criterion=None,\n",
    "        batch_size=64,\n",
    "        learning_rate=1e-4,\n",
    "        num_epochs=10,\n",
    "        model_string=\"checkpoint\"\n",
    "        ):\n",
    "    if criterion is None:\n",
    "        print(f\"Using MSE loss as default loss function\")\n",
    "        criterion = torch.nn.MSELoss()\n",
    "    \n",
    "    model_save_path = os.path.join(CURRENT_DIR, f'models/{model_string}.pt')\n",
    "    print(f\"Checkpoint save path: {model_save_path}\")\n",
    "\n",
    "    fig, ax = init_training_loss_visualiser(num_epochs)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "    # Run the training loop\n",
    "    i = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        print(f\"Epoch {epoch} starts\")\n",
    "        total_loss = 0\n",
    "        print(\"--------Training\")\n",
    "        for data in train_dataloader:\n",
    "            # Get batch of data\n",
    "            inputs, labels = data\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Perform forward pass\n",
    "            #print(inputs.shape, labels.shape)\n",
    "            network_output = model.forward(inputs)\n",
    "            #print(network_output.shape)\n",
    "            # Compute loss\n",
    "            loss = criterion(network_output, labels)\n",
    "            # Ensure no funny numerics\n",
    "            assert not torch.isnan(loss).any()\n",
    "            # Perform backward pass\n",
    "            loss.backward()\n",
    "            # \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 10.0)\n",
    "            # Perform optimization\n",
    "            optimizer.step()\n",
    "            # Print statistics\n",
    "            total_loss += loss.detach().item()\n",
    "            i += 1\n",
    "            if i % 100 == 0:\n",
    "                average_loss = total_loss / i\n",
    "                rms = np.sqrt(average_loss)\n",
    "                print('-----------------Average loss/RMS error after mini-batch %5d, epoch %d : %.3f, %.3f' % (i, epoch, average_loss, rms))\n",
    "        train_losses.append(total_loss / i)\n",
    "\n",
    "        # Evaluate on validation set at end of each epoch\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "\n",
    "        for data in eval_dataloader:\n",
    "            # Get batch of data\n",
    "            inputs, labels = data\n",
    "            # Perform forward pass\n",
    "            network_output = model.forward(inputs)\n",
    "            # Compute loss\n",
    "            loss = criterion(network_output, labels)\n",
    "            total_loss += loss.detach().item()\n",
    "\n",
    "        average_loss = total_loss / len(eval_dataloader)\n",
    "        rms = np.sqrt(average_loss)\n",
    "        print(\"--------Evaluation\")\n",
    "        print('-----------------Total loss/RMS error after epoch %5d: %.3f, %.3f' % (epoch, average_loss, rms))\n",
    "        \n",
    "        val_losses.append(average_loss)\n",
    "        visualise_loss(fig, ax, train_losses, val_losses)\n",
    "            \n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    \n",
    "    print(f\"-----------------saved epoch {epoch} to \", model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474bc811",
   "metadata": {},
   "source": [
    "We implement a training loop to train autoencoder architectures to reproduce the occupancy grid. Through this we hope to create encoders that have effectively captured the spatial information of the occupancy grids, and which we can use to create latent representations of the occupancy grids in our other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb90fa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def train_autoencoder(\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        eval_dataloader,\n",
    "        criterion=None,\n",
    "        batch_size=64,\n",
    "        learning_rate=1e-4,\n",
    "        num_epochs=10,\n",
    "        model_string=\"checkpoint\"\n",
    "        ):\n",
    "    if criterion is None:\n",
    "        print(f\"Using MSE loss as default loss function\")\n",
    "        criterion = torch.nn.MSELoss()\n",
    "    \n",
    "    model_save_path = os.path.join(CURRENT_DIR, f'models/{model_string}.pt')\n",
    "    print(f\"Checkpoint save path: {model_save_path}\")\n",
    "\n",
    "    fig, ax = init_training_loss_visualiser(num_epochs)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "    # Run the training loop\n",
    "    i = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        print(f\"Epoch {epoch} starts\")\n",
    "        total_loss = 0\n",
    "        print(\"--------Training\")\n",
    "        for data in train_dataloader:\n",
    "            # Get batch of data\n",
    "            occ_grids = data\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Perform forward pass\n",
    "            soft_network_output = model.forward(occ_grids)\n",
    "            # Compute loss\n",
    "            soft_occ_grids = F.sigmoid(occ_grids)\n",
    "            loss = criterion(soft_network_output, soft_occ_grids)\n",
    "            # Ensure no funny numerics\n",
    "            assert not torch.isnan(loss).any()\n",
    "            # Perform backward pass\n",
    "            loss.backward()\n",
    "            # \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 10.0)\n",
    "            # Perform optimization\n",
    "            optimizer.step()\n",
    "            # Print statistics\n",
    "            total_loss += loss.detach().item()\n",
    "            i += 1\n",
    "            if i % 100 == 0:\n",
    "                average_loss = total_loss / i\n",
    "                rms = np.sqrt(average_loss)\n",
    "                print('-----------------Average loss/RMS error after mini-batch %5d, epoch %d : %.3f, %.3f' % (i, epoch, average_loss, rms))\n",
    "        train_losses.append(total_loss / i)\n",
    "\n",
    "        # Evaluate on validation set at end of each epoch\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "\n",
    "        for data in eval_dataloader:\n",
    "            # Get batch of data\n",
    "            occ_grids = data\n",
    "            # Perform forward pass\n",
    "            soft_occ_grids = F.sigmoid(occ_grids)\n",
    "            soft_network_output = model.forward(occ_grids)\n",
    "            # Compute loss\n",
    "            loss = criterion(soft_network_output, soft_occ_grids)\n",
    "            total_loss += loss.detach().item()\n",
    "\n",
    "        average_loss = total_loss / len(eval_dataloader)\n",
    "        rms = np.sqrt(average_loss)\n",
    "        print(\"--------Evaluation\")\n",
    "        print('-----------------Total loss/RMS error after epoch %5d: %.3f, %.3f' % (epoch, average_loss, rms))\n",
    "        \n",
    "        val_losses.append(average_loss)\n",
    "        visualise_loss(fig, ax, train_losses, val_losses)\n",
    "            \n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    \n",
    "    print(f\"-----------------saved epoch {epoch} to \", model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7103ce93",
   "metadata": {},
   "source": [
    "We implement a training loop to train recurrent architectures that can retain memory of past waypoints while regressing future waypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc0e86c",
   "metadata": {
    "id": "4bc0e86c"
   },
   "outputs": [],
   "source": [
    "def normalize_gradient(net):\n",
    "    grad_norm_sq=0\n",
    "    for p in net.parameters():\n",
    "        grad_norm_sq += p.grad.data.norm()**2\n",
    "\n",
    "    grad_norm=math.sqrt(grad_norm_sq)\n",
    "    if grad_norm<1e-4:\n",
    "        net.zero_grad()\n",
    "        print('grad norm close to zero')\n",
    "    else:    \n",
    "        for p in net.parameters():\n",
    "             p.grad.data.div_(grad_norm)\n",
    "\n",
    "    return grad_norm\n",
    "\n",
    "def train_recurrent(\n",
    "        model,\n",
    "        network_hidden_size,\n",
    "        train_dataloaders,\n",
    "        eval_dataloader,\n",
    "        criterion=None,\n",
    "        batch_size=64,\n",
    "        learning_rate=1e-4,\n",
    "        num_epochs=10,\n",
    "        model_string=\"checkpoint\"\n",
    "        ):\n",
    "    if criterion is None:\n",
    "        print(f\"Using MSE loss as default loss function\")\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        \n",
    "    fig, ax = init_training_loss_visualiser(num_epochs)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "        \n",
    "    model_save_path = os.path.join(CURRENT_DIR, f'models/{model_string}.pt')\n",
    "    print(f\"Checkpoint save path: {model_save_path}\")\n",
    "        \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "    # Run the training loop\n",
    "    i = 0\n",
    "    frac = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        print(f\"Epoch {epoch} starts\")\n",
    "        total_loss = 0\n",
    "        print(\"--------Training\")\n",
    "\n",
    "        for dataloader, path_length in train_dataloaders:\n",
    "            print(\"Training on dataset with path length: \", path_length)\n",
    "            for data in dataloader:\n",
    "                # Get batch of data\n",
    "                inputs, labels = data\n",
    "                # Zero the gradients\n",
    "                optimizer.zero_grad()\n",
    "                # Initialise the hidden state\n",
    "                h_init = torch.zeros(1, inputs[0].shape[0], network_hidden_size).to(DEVICE)\n",
    "                # Perform forward pass\n",
    "                output, _ = model.forward(inputs, h_init, initialise=True)\n",
    "                # Compute loss\n",
    "                loss = criterion(output, labels)\n",
    "                # Ensure no funny numerics\n",
    "                assert not torch.isnan(loss).any()\n",
    "                # Perform backward pass\n",
    "                loss.backward()\n",
    "                # Clip gradients to prevent explosion\n",
    "                normalize_gradient(model)\n",
    "                # Perform optimization\n",
    "                optimizer.step()\n",
    "\n",
    "                # Print statistics\n",
    "                total_loss += loss.detach().item()\n",
    "                i += 1\n",
    "                frac += float(inputs[0].shape[0]) / float(batch_size)\n",
    "                if i % 100 == 0:\n",
    "                    #average_loss = total_loss / frac\n",
    "                    average_loss = total_loss / i\n",
    "                    rms = np.sqrt(average_loss)\n",
    "                    print('-----------------Average loss/RMS error after mini-batch %5d, epoch %d : %.3f, %.3f' % (i, epoch, average_loss, rms))\n",
    "                    \n",
    "        train_losses.append(total_loss / i)\n",
    "        \n",
    "        # Evaluate on validation set at end of each epoch\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "\n",
    "        for data in eval_dataloader:\n",
    "            # Get batch of data\n",
    "            inputs, labels = data\n",
    "            # Perform forward pass\n",
    "            h_init = torch.zeros(1, inputs[0].shape[0], network_hidden_size).to(DEVICE)\n",
    "            network_output, _ = model.forward(inputs, h_init, initialise=True)\n",
    "            # Compute loss\n",
    "            loss = criterion(network_output, labels)\n",
    "            total_loss += loss.detach().item()\n",
    "\n",
    "        average_loss = total_loss / len(eval_dataloader)\n",
    "        rms = np.sqrt(average_loss)\n",
    "        print(\"--------Evaluation\")\n",
    "        print('-----------------Total loss/RMS error after epoch %5d: %.3f, %.3f' % (epoch, average_loss, rms))\n",
    "        \n",
    "        val_losses.append(average_loss)\n",
    "        visualise_loss(fig, ax, train_losses, val_losses)\n",
    "                    \n",
    "        # save at each epoch\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'rnn_hidden_size': model.latent_size\n",
    "        }, model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d572a7e",
   "metadata": {
    "id": "5d572a7e"
   },
   "source": [
    "## Training feedforward architectures\n",
    "\n",
    "Implement some utilities for loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d181de7d",
   "metadata": {
    "executionInfo": {
     "elapsed": 392,
     "status": "ok",
     "timestamp": 1635871785033,
     "user": {
      "displayName": "Joel Loo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11818416161416341433"
     },
     "user_tz": -480
    },
    "id": "d181de7d"
   },
   "outputs": [],
   "source": [
    "def init_feedforward_dataloaders(dataset_cls, train_frac, batch_size):\n",
    "    train_val_dataset = dataset_cls.load_dataset_from_file(os.path.join(CURRENT_DIR, 'dataset/data_waypoints.json'), device=DEVICE)\n",
    "    train_size = int(len(train_val_dataset) * train_frac)\n",
    "    \n",
    "    train_set, val_set = torch.utils.data.random_split(train_val_dataset, [train_size, len(train_val_dataset) - train_size])\n",
    "    #train_set = torch.utils.data.Subset(train_val_dataset, np.arange(train_size))\n",
    "    #val_set = torch.utils.data.Subset(train_val_dataset, np.arange(train_size, len(train_val_dataset)))\n",
    "    test_set = dataset_cls.load_dataset_from_file(os.path.join(CURRENT_DIR, 'test_dataset/data_waypoints.json'), device=DEVICE)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    eval_dataloader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_dataloader, eval_dataloader, test_dataloader, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc11256",
   "metadata": {
    "id": "acc11256"
   },
   "source": [
    "Train a vanilla MLP to predict next waypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47ed1f4",
   "metadata": {
    "id": "e47ed1f4"
   },
   "outputs": [],
   "source": [
    "# Parameters specific to this instantiation of the model and its training\n",
    "model_string = \"MLP_test3\"\n",
    "train_frac = 0.9 # Proportion of dataset to use as train set (otherwise will be validation)\n",
    "batch_size = 64\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 10\n",
    "criterion = torch.nn.MSELoss()\n",
    "model_cls = MLPModel\n",
    "train_loader, eval_loader, test_loader, test_raw = init_feedforward_dataloaders(MLPDataset, train_frac, batch_size)\n",
    "\n",
    "# Train\n",
    "model = model_cls().to(DEVICE)\n",
    "train_feedforward(model, train_loader, eval_loader, criterion=criterion, batch_size=batch_size, \n",
    "                  learning_rate=learning_rate, num_epochs=num_epochs, model_string=model_string)\n",
    "\n",
    "# Evaluate\n",
    "evaluate_test_metrics(test_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39c23b5",
   "metadata": {
    "id": "a39c23b5"
   },
   "outputs": [],
   "source": [
    "# Visualize random prediction on test set\n",
    "visualize_model_output(test_raw, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63406d01",
   "metadata": {
    "id": "63406d01"
   },
   "source": [
    "Train a vanilla CNN with a fully-connected head to predict next waypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a52a3f",
   "metadata": {
    "id": "b9a52a3f"
   },
   "outputs": [],
   "source": [
    "# Parameters specific to this instantiation of the model and its training\n",
    "model_string = \"MLPCNN_test\"\n",
    "train_frac = 0.9 # Proportion of dataset to use as train set (otherwise will be validation)\n",
    "batch_size = 64\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 1\n",
    "criterion = torch.nn.MSELoss()\n",
    "model_cls = MLPCNNModel\n",
    "train_loader, eval_loader, test_loader, test_raw = init_feedforward_dataloaders(MLPCNNDataset, train_frac, batch_size)\n",
    "\n",
    "# Train\n",
    "model = model_cls().to(DEVICE)\n",
    "train_feedforward(model, train_loader, eval_loader, criterion=criterion, batch_size=batch_size, \n",
    "                  learning_rate=learning_rate, num_epochs=num_epochs, model_string=model_string)\n",
    "\n",
    "# Evaluate\n",
    "evaluate_test_metrics(test_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc8b342",
   "metadata": {
    "id": "2bc8b342"
   },
   "outputs": [],
   "source": [
    "# Visualize random prediction on test set\n",
    "visualize_model_output(test_raw, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483630dd",
   "metadata": {},
   "source": [
    "Train an MLP autoencoder to reconstruct the occupancy grid, with the aim of extracting the encoder and using it to generate a latent representation of the grid in other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c31a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters specific to this instantiation of the model and its training\n",
    "model_string = \"MLPAutoencoder\"\n",
    "train_frac = 0.9 # Proportion of dataset to use as train set (otherwise will be validation)\n",
    "batch_size = 64\n",
    "learning_rate = 1e-2\n",
    "num_epochs = 10\n",
    "criterion = torch.nn.BCELoss()\n",
    "model_cls = MLPAutoencoderModel\n",
    "train_loader, eval_loader, test_loader, test_raw = init_feedforward_dataloaders(MLPAutoencoderDataset, train_frac, batch_size)\n",
    "\n",
    "# Train\n",
    "model = model_cls().to(DEVICE)\n",
    "train_autoencoder(model, train_loader, eval_loader, criterion=criterion, batch_size=batch_size, \n",
    "                  learning_rate=learning_rate, num_epochs=num_epochs, model_string=model_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17NfRhE_DS4A",
   "metadata": {
    "id": "17NfRhE_DS4A"
   },
   "source": [
    "Test a MLP-CNN architecture, where the convolutional encoder (CNN) has first been trained as part of an autoencoder to learn a compressed representation of the map. The encoder is pretrained on a different dataset of occupancy grids, and is finetuned here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FyuJ6pr3D3ZH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1058241,
     "status": "ok",
     "timestamp": 1635873076101,
     "user": {
      "displayName": "Joel Loo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11818416161416341433"
     },
     "user_tz": -480
    },
    "id": "FyuJ6pr3D3ZH",
    "outputId": "ccfb20ef-5e44-40eb-fc32-3f06e9e53a69"
   },
   "outputs": [],
   "source": [
    "# Parameters specific to this instantiation of the model and its training\n",
    "model_string = \"MLPPretrainedConvEncoder_test2\"\n",
    "train_frac = 0.9 # Proportion of dataset to use as train set (otherwise will be validation)\n",
    "batch_size = 64\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 20\n",
    "criterion = torch.nn.MSELoss()\n",
    "model_cls = MLPCNNModel2\n",
    "train_loader, eval_loader, test_loader, test_raw = init_feedforward_dataloaders(MLPCNNDataset, train_frac, batch_size)\n",
    "\n",
    "# Train\n",
    "model = model_cls(pretrained_model_path=\"models/conv_encoder_7.pt\").to(DEVICE)\n",
    "train_feedforward(model, train_loader, eval_loader, criterion=criterion, batch_size=batch_size, \n",
    "                  learning_rate=learning_rate, num_epochs=num_epochs, model_string=model_string)\n",
    "\n",
    "# Evaluate\n",
    "evaluate_test_metrics(test_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gaoEvq6fEFaZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 838
    },
    "executionInfo": {
     "elapsed": 1002,
     "status": "ok",
     "timestamp": 1635873308525,
     "user": {
      "displayName": "Joel Loo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11818416161416341433"
     },
     "user_tz": -480
    },
    "id": "gaoEvq6fEFaZ",
    "outputId": "cc4653be-15cc-4e0d-8df5-e643363ef70f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize random prediction on test set\n",
    "visualize_model_output(test_raw, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3165f2df",
   "metadata": {
    "id": "3165f2df"
   },
   "source": [
    "## Training recurrent architectures\n",
    "\n",
    "Implement utilities for loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053f9b94",
   "metadata": {
    "id": "053f9b94"
   },
   "outputs": [],
   "source": [
    "def init_recurrent_dataloaders(cls, train_frac, batch_size):\n",
    "    train_loaders, data_loader = MLPRNNDataloader.load_dataset_from_file(\n",
    "        os.path.join(CURRENT_DIR, 'dataset/data_path.json'), batch_size, train_frac,\n",
    "        shuffle=True, drop_last=True, device=DEVICE\n",
    "    )\n",
    "    \n",
    "    TEST_DATA_PATH = os.path.join(CURRENT_DIR, 'test_dataset/data_path.json')\n",
    "    test_loader, _ = MLPRNNDataloader.load_dataset_from_file(TEST_DATA_PATH, batch_size, 1.0, \n",
    "                                                             shuffle=False, drop_last=False, device=DEVICE)\n",
    "    test_set = cls.load_dataset_from_file(TEST_DATA_PATH, device=DEVICE)\n",
    "    \n",
    "    return train_loaders, data_loader, test_loader, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2f66be",
   "metadata": {
    "id": "9d2f66be"
   },
   "source": [
    "Implement a basic RNN to predict the path from start to goal. At each timestep, the input to the RNN is the concatenation of its current position and the goal position, and the output is the next waypoint it should head towards. The RNN is conditioned on the obstacle grid - this is done by encoding the obstacle grid in a feature vector using an MLP, and adding the feature vector to the RNN's initial hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48936469",
   "metadata": {
    "id": "48936469"
   },
   "outputs": [],
   "source": [
    "# Parameters specific to this instantiation of the model and its training\n",
    "model_string = \"MLPRNN_test7\"\n",
    "train_frac = 0.9 # Proportion of dataset to use as train set (otherwise will be validation)\n",
    "batch_size = 64\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 10\n",
    "criterion = torch.nn.MSELoss()\n",
    "model_cls = MLPRNNModel\n",
    "network_hidden_size = 128\n",
    "train_loaders, eval_loader, test_loader, test_raw = init_recurrent_dataloaders(MLPRNNDataset, train_frac, batch_size)\n",
    "\n",
    "# Train\n",
    "model = model_cls(network_hidden_size).to(DEVICE)\n",
    "train_recurrent(model, network_hidden_size, train_loaders, eval_loader, criterion=criterion, batch_size=batch_size, \n",
    "                learning_rate=learning_rate, num_epochs=num_epochs, model_string=model_string)\n",
    "\n",
    "# Evaluate\n",
    "evaluate_test_metrics(test_loader, model, criterion, is_recurrent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92e079b",
   "metadata": {
    "id": "c92e079b"
   },
   "outputs": [],
   "source": [
    "# Visualize random prediction on test set\n",
    "visualize_model_output(test_raw, model, is_recurrent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f9875d",
   "metadata": {
    "id": "71f9875d"
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "We present here the consolidated results from training and evaluating the proposed network architectures. We also focus on running evaluating the performance of our models on a full motion planning test set-up. Our previous evaluation focused on predicting waypoints one step ahead. In contrast we will now run the inference iteratively to continuously generate waypoints based on the last generated waypoint, and attempt to generate a full path linking the start to the end goal.\n",
    "\n",
    "## Evaluation of proposed network architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c956aef8",
   "metadata": {
    "id": "c956aef8"
   },
   "source": [
    "We will evaluate the performance of the trained network architectures using the following criteria:\n",
    "* Validation and test loss. Since we train with a mean-squared error loss between the regressed waypoints and ground truth waypoints, the square root of the validation loss indicates how far on average the regressed waypoints are from ground truth.\n",
    "* Success rate in generating a next waypoint that is directly reachable from current position and not obstructed by obstacles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c660cc30",
   "metadata": {},
   "source": [
    "|                                       \t| MLP \t|      CNN      \t| RNN \t|\n",
    "|---------------------------------------\t|:---:\t|:-------------:\t|:---:\t|\n",
    "| Validation loss (MSE/RMS error)       \t|     \t| 0.101 / 0.318 \t|     \t|\n",
    "| Testing loss (MSE/RMS error)          \t|     \t|  0.3 / 0.548  \t|     \t|\n",
    "| Next waypoint generation success rate \t|     \t|               \t|     \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7de464",
   "metadata": {
    "id": "ea7de464"
   },
   "source": [
    "## Motion planning test\n",
    "\n",
    "We created a test dataset of 100 environments. In each environment we have sampled start and goal positions, and computed a feasible path between them using A* search.\n",
    "\n",
    "We will evaluate the performance of the motion planning test based on two criteria:\n",
    "* Success rate at generating a feasible path to goal. Success is defined as a generated waypoint reaching the goal within a given tolerance of 0.2m within 15 steps, while not colliding with obstacles along the way.\n",
    "* Length of predicted path compared to ground truth path length. This is given by the path length ratio $$\\frac{l_i}{\\max(p_i, l_i)}$$. $l_i$ is the shortest path length and $p_i$ is the predicted path length. The shortest path length will be approximated using the length of the ground truth path computed. \n",
    "\n",
    "Both of these metrics can also be unified into a single metric termed the success weighted by (normalized inverse) path length (SPL) [1]. Specifically, the SPL is\n",
    "$$ \\frac{1}{N} \\sum\\limits_{i=1}^{N} S_i \\frac{l_i}{\\max(p_i, l_i)} $$, where $N$ is the number of motion planning test episodes and $S_i$ is an indicator variable which is 1 if the test succeeds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95a0f71",
   "metadata": {},
   "source": [
    "First, we implement the utilities for running motion planning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c7ece1",
   "metadata": {
    "executionInfo": {
     "elapsed": 307,
     "status": "ok",
     "timestamp": 1635873351866,
     "user": {
      "displayName": "Joel Loo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11818416161416341433"
     },
     "user_tz": -480
    },
    "id": "e5c7ece1"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def single_motion_planning_test(model, dataset, idx, max_iters=5, is_recurrent=False, check_collision=False):\n",
    "    status = 'SUCCESS'\n",
    "    inputs, _ = dataset[idx]\n",
    "    _, goal_pos, occ_grid, gt = dataset.get_raw_data(idx)\n",
    "    maze = Maze2D.load_small_occupancy_grid(occ_grid)\n",
    "    \n",
    "    current_pos = np.array(gt[0], dtype=np.float32)\n",
    "    goal_pos = np.array(goal_pos)\n",
    "    it = 0\n",
    "    predicted_path = [gt[0]]\n",
    "    hidden_state = None if not is_recurrent else model.init_with_zeros(1)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        while np.linalg.norm(current_pos - goal_pos) > 0.2:\n",
    "            if it >= max_iters:\n",
    "                status = 'NOT-REACH'\n",
    "                break\n",
    "\n",
    "            formatted = dataset.format_input_for_motion_planning(inputs, torch.from_numpy(current_pos).to(DEVICE))\n",
    "            if is_recurrent:\n",
    "                output, hidden_state = model.forward(formatted, hidden_state, initialise=(it==0))\n",
    "                predicted = torch.squeeze(output.detach()).cpu().numpy().flatten()\n",
    "            else:\n",
    "                output = model.forward(formatted)\n",
    "                predicted = output.detach().cpu().numpy().flatten()\n",
    "            \n",
    "            predicted_path.append(predicted.tolist())\n",
    "            current_pos = predicted\n",
    "            \n",
    "            if check_collision:\n",
    "                if ( (len(predicted_path) == 1 and not maze.is_state_valid(predicted)) or\n",
    "                     (len(predicted_path) > 1 and not utils.is_edge_free(maze, predicted_path[-2], predicted_path[-1])) ):\n",
    "                    status = 'COLLIDED'\n",
    "                    print('COLLIDED')\n",
    "                    break\n",
    "            \n",
    "            it += 1\n",
    "            \n",
    "        return status, np.array(predicted_path), occ_grid, gt\n",
    "    \n",
    "def test_over_data(model, data, dataset_cls, number=100, is_recurrent=False, check_collision=False):\n",
    "    success_count = 0.0\n",
    "    data = copy.deepcopy(data)\n",
    "    random.shuffle(data)\n",
    "    count = 0\n",
    "    test_set = dataset_cls(data, device=DEVICE)\n",
    "    path_length_ratio = 0.0\n",
    "    number = len(data) if number < 0 else number\n",
    "    \n",
    "    for idx in range(number):\n",
    "        if count % 10 == 0:\n",
    "          print(\"Tested \", count)\n",
    "        count += 1\n",
    "        status, predicted_path, _, gt_path = single_motion_planning_test(\n",
    "            model, test_set, idx, is_recurrent=is_recurrent, check_collision=check_collision)\n",
    "        if status == 'SUCCESS':\n",
    "            success_count += 1\n",
    "            predicted_path = np.array(predicted_path)\n",
    "            gt_path = np.array(gt_path)\n",
    "            predicted_path_length = np.sum(np.linalg.norm(predicted_path[1:,:] - predicted_path[:-1,:], axis=1))\n",
    "            gt_path_length = np.sum(np.linalg.norm(gt_path[1:,:] - gt_path[:-1,:], axis=1))\n",
    "            path_length_ratio += gt_path_length / max(predicted_path_length, gt_path_length)\n",
    "    return success_count / number, (0 if success_count == 0 else path_length_ratio / success_count)\n",
    "\n",
    "def load_feedforward_model(model_cls, model_string):\n",
    "    model = model_cls()\n",
    "    model_dir = os.path.join(CURRENT_DIR, 'models')\n",
    "    model.load_state_dict(torch.load(os.path.join(model_dir, model_string + \".pt\")))\n",
    "    model.to(DEVICE)\n",
    "    return model\n",
    "\n",
    "def load_recurrent_model(model_cls, model_string):\n",
    "    model_dir = os.path.join(CURRENT_DIR, 'models')\n",
    "    model_dict = torch.load(os.path.join(model_dir, model_string + \".pt\"))\n",
    "    model_hidden_size = model_dict['rnn_hidden_size']\n",
    "    model = model_cls(model_hidden_size)\n",
    "    model.load_state_dict(model_dict['model_state_dict'])\n",
    "    model.to(DEVICE)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4c530f",
   "metadata": {
    "id": "1d4c530f"
   },
   "source": [
    "### Visualising individual motion planning tests\n",
    "\n",
    "We visualise the planned paths iteratively generated by our proposed network architectures on randomly selected test cases from our test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8b251e",
   "metadata": {},
   "source": [
    "1. We test a simple CNN-based model consisting of a convolutional encoder with a fully-connected head for regressing the next waypoint. The convolutional encoder is extracted from an autoencoder trained to reconstruct the obstacle grid, and is finetuned with the fully-connected head on our waypoint dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66lhQFLQpsmJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 717,
     "status": "ok",
     "timestamp": 1635873493080,
     "user": {
      "displayName": "Joel Loo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11818416161416341433"
     },
     "user_tz": -480
    },
    "id": "66lhQFLQpsmJ",
    "outputId": "2fd84f6f-844b-4de8-9cc7-4edcee6c489f"
   },
   "outputs": [],
   "source": [
    "### Motion planning test with CNN based model ###\n",
    "# Load data once, so we do not have to load repeatedly\n",
    "test_set = MLPCNNDataset.load_dataset_from_file(os.path.join(CURRENT_DIR, 'test_dataset/data_path.json'), device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dH-s-Ojp0ps",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 872
    },
    "executionInfo": {
     "elapsed": 1033,
     "status": "ok",
     "timestamp": 1635873611259,
     "user": {
      "displayName": "Joel Loo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11818416161416341433"
     },
     "user_tz": -480
    },
    "id": "7dH-s-Ojp0ps",
    "outputId": "32f67b16-c24f-4d0f-806b-7c59ba5adac7"
   },
   "outputs": [],
   "source": [
    "# Select a random test case, perform motion planning with iterative inferences and visualise results\n",
    "model = load_feedforward_model(MLPCNNModel2, \"MLPPretrainedConvEncoder\")\n",
    "idx = np.random.randint(len(test_set))\n",
    "status, predicted_path, occ_grid, gt_path = single_motion_planning_test(\n",
    "    model, test_set, idx, check_collision=False)\n",
    "print(f'Status: {status}')\n",
    "visualize_data(occ_grid, gt_path[0], gt_path[-1], gt_path[1:], predicted_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4fde5a",
   "metadata": {},
   "source": [
    "2. We test a simple RNN, that takes at each timestep an input consisting of the concatenated goal and current position. The obstacle grid is encoded into the initial hidden state of the RNN using an MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab9f983",
   "metadata": {
    "id": "6ab9f983"
   },
   "outputs": [],
   "source": [
    "### Motion planning test with RNN ###\n",
    "# Load data once, so we do not have to load repeatedly\n",
    "test_set = MLPRNNDataset.load_dataset_from_file(os.path.join(CURRENT_DIR, 'test_dataset/data_path.json'), device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24f4872",
   "metadata": {
    "id": "a24f4872",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Select a random test case, perform motion planning with iterative inferences and visualise results\n",
    "model = load_recurrent_model(MLPRNNModel, \"MLPRNN_test4\")\n",
    "idx = np.random.randint(len(test_set))\n",
    "status, predicted_path, occ_grid, gt_path = single_motion_planning_test(\n",
    "    model, test_set, idx, is_recurrent=True, check_collision=True, max_iters=10)\n",
    "print(f'Status: {status}')\n",
    "visualize_data(occ_grid, gt_path[0], gt_path[-1], gt_path[1:], predicted_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41195536",
   "metadata": {
    "id": "41195536"
   },
   "source": [
    "### Overall performance on test dataset\n",
    "\n",
    "We evaluate the performance of our networks at motion planning, over the entire test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc5f19a",
   "metadata": {
    "id": "6dc5f19a"
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(CURRENT_DIR, 'test_dataset/data_path.json')) as _file:\n",
    "    data = json.load(_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d105d573",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Motion planning test with MLP ###\n",
    "# Provide the model-specific parameters\n",
    "model_cls = MLPModel\n",
    "dataset_cls = MLPDataset\n",
    "\n",
    "model = load_feedforward_model(MLPModel, \"MLP_test3\")\n",
    "success_rate, path_length_ratio = test_over_data(model, data, dataset_cls, number=-1, is_recurrent=False, check_collision=True)\n",
    "print(\"Success rate: \", success_rate, \" Path length ratio: \", path_length_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38016011",
   "metadata": {
    "id": "38016011"
   },
   "outputs": [],
   "source": [
    "### Motion planning test with RNN ###\n",
    "# Provide the model-specific parameters\n",
    "model_cls = MLPRNNModel\n",
    "dataset_cls = MLPRNNDataset\n",
    "\n",
    "# Evaluate performance on test set\n",
    "model = load_recurrent_model(MLPRNNModel, \"MLPRNN_test4\")\n",
    "success_rate, path_length_ratio = test_over_data(model, data, dataset_cls, number=-1, is_recurrent=True, check_collision=True)\n",
    "print(success_rate, path_length_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3d2929",
   "metadata": {
    "id": "bc3d2929"
   },
   "source": [
    "# Discussion of results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87dca85",
   "metadata": {
    "id": "b87dca85"
   },
   "source": [
    "TODO: Summarise the experimental results and provide suggestions for the observed behaviour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec02905",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c44653",
   "metadata": {},
   "source": [
    "[1] P. Anderson, A. Chang, D.S. Chaplot,A.  Dosovitskiy, S. Gupta, V. Koltun, J. Kosecka, J. Malik, R. Mottaghi, M. Savva, A.R. Zamir. On Evaluation of Embodied Navigation Agents. arXiv preprint 1807.06757, 2018"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "3hN-efIwnVxD",
    "f313fd07",
    "aeaa7faa",
    "1c44b554",
    "c0a7a066",
    "c8c60ebb",
    "bc3d2929"
   ],
   "name": "our-notebook.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "b1513172bc75592431e687543e7a7e71518769f9e336f9ec823ad71ba40cd915"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
