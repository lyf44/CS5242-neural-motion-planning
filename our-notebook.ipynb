{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3hN-efIwnVxD",
   "metadata": {
    "id": "3hN-efIwnVxD"
   },
   "source": [
    "# Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bcb08f",
   "metadata": {},
   "source": [
    "To get started with running on Colab, run the following script after changing CURRENT_DIR to the correct path to the project folder. **Note**: Colab does not fully support interactive Matplotlib plots, so some functions like loss visualisation during training may not work fully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elzySajjkdbz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23039,
     "status": "ok",
     "timestamp": 1635870709074,
     "user": {
      "displayName": "Joel Loo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11818416161416341433"
     },
     "user_tz": -480
    },
    "id": "elzySajjkdbz",
    "outputId": "7e3218e6-7428-4189-9d82-7d3bb2238699"
   },
   "outputs": [],
   "source": [
    "# If running on Colab\n",
    "%matplotlib inline\n",
    "\n",
    "CURRENT_DIR = '/content/drive/My Drive/Classes/CS5242 project'\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bc75cd",
   "metadata": {},
   "source": [
    "If running the notebook locally (e.g. Jupyter or Jupyter Labs), run the following script instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eY3chhRdkjJq",
   "metadata": {
    "id": "eY3chhRdkjJq"
   },
   "outputs": [],
   "source": [
    "# If running locally\n",
    "%matplotlib notebook\n",
    "CURRENT_DIR = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00283ef",
   "metadata": {},
   "source": [
    "Run the following script regardless of whether you are running locally or on Colab, to set up your paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "A9pF3dgaqQNl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 699,
     "status": "ok",
     "timestamp": 1635870709766,
     "user": {
      "displayName": "Joel Loo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11818416161416341433"
     },
     "user_tz": -480
    },
    "id": "A9pF3dgaqQNl",
    "outputId": "c7eed81e-26c2-494e-90e3-270657d747fe"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir(CURRENT_DIR)\n",
    "sys.path.append(CURRENT_DIR)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f313fd07",
   "metadata": {
    "id": "f313fd07"
   },
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaa7faa",
   "metadata": {
    "id": "aeaa7faa"
   },
   "source": [
    "## Environment and robot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36daf550",
   "metadata": {
    "id": "36daf550"
   },
   "source": [
    "Our robot is a shaped as a 2d planar box with size equals to 0.2 meters. It can move simultaneously both horizontally and vertically. Therefore, its configuration space is a 2d rectangle which size is determined by the maze size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8c5689",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1635870709766,
     "user": {
      "displayName": "Joel Loo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11818416161416341433"
     },
     "user_tz": -480
    },
    "id": "0f8c5689"
   },
   "outputs": [],
   "source": [
    "class MyPlanarRobot():\n",
    "    def __init__(self, base_xy_bounds=5.0) -> None:\n",
    "        self.num_dim = 2\n",
    "        self.joint_idx=[0,1]\n",
    "        self.size = 0.2\n",
    "\n",
    "        self.joint_bounds = []\n",
    "        self.joint_bounds.append([-base_xy_bounds, base_xy_bounds]) # x\n",
    "        self.joint_bounds.append([-base_xy_bounds, base_xy_bounds]) # y\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def get_joint_bounds(self):\n",
    "        return self.joint_bounds\n",
    "\n",
    "    def get_joint_lower_bounds(self):\n",
    "        robot_bounds_low = [bound[0] for bound in self.joint_bounds]\n",
    "        return robot_bounds_low\n",
    "\n",
    "    def get_joint_higher_bounds(self):\n",
    "        robot_bounds_high = [bound[1] for bound in self.joint_bounds]\n",
    "        return robot_bounds_high\n",
    "\n",
    "    def get_cur_state(self):\n",
    "        return self.state\n",
    "\n",
    "    def set_state(self, state):\n",
    "        self.state = state\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = [0] * self.num_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83be3dd3",
   "metadata": {
    "id": "83be3dd3"
   },
   "source": [
    "Our environment is a 2d maze with size 5m * 5m. It is filled with random generated square obstacles of fixed size 1m * 1m.  The difficulties of the maze can be manipulated by altering the number of obstacles present. The maze can be visualized by an occupancy grid with resolution 0.5m, making it essentially an image of size 10 x 10. \n",
    "\n",
    "To facilitate path planning, the maze class also contains code to sample valid start and goals of the point robot and perform collision checking for a given robot configuration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dd910d",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1635870709767,
     "user": {
      "displayName": "Joel Loo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11818416161416341433"
     },
     "user_tz": -480
    },
    "id": "c8dd910d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "import sys\n",
    "import os.path as osp\n",
    "\n",
    "# -------------- Settings ----------------\n",
    "RANDOM = True\n",
    "TOTAL_START_GOAL_CNT = 50\n",
    "MAZE_SIZE = 5\n",
    "OCC_GRID_RESOLUTION = 0.1\n",
    "SMALL_OCC_GRID_RESLUTION = 0.5\n",
    "\n",
    "OCC_GRID_RESOLUTION_DIAG = math.sqrt(2 * OCC_GRID_RESOLUTION**2)\n",
    "\n",
    "class Maze2D():\n",
    "    def __init__(self):\n",
    "        self.obstacles = []\n",
    "\n",
    "        # load robot\n",
    "        robot = MyPlanarRobot(base_xy_bounds = MAZE_SIZE / 2.0)\n",
    "        self.robot = robot\n",
    "\n",
    "        # 2d occupancy grid\n",
    "        self.occ_grid_size = int(MAZE_SIZE / OCC_GRID_RESOLUTION)\n",
    "        self.occ_grid = np.zeros((self.occ_grid_size, self.occ_grid_size), dtype=np.uint8)\n",
    "        self.small_occ_grid_size = int(MAZE_SIZE / SMALL_OCC_GRID_RESLUTION)\n",
    "        self.sdf = np.zeros((self.occ_grid_size, self.occ_grid_size), dtype=np.float32)\n",
    "\n",
    "        # clear obstacles\n",
    "        self.clear_obstacles()\n",
    "\n",
    "        # add surrounding walls\n",
    "        half_size = MAZE_SIZE / 2.0\n",
    "        # add wall\n",
    "        self.add_box([half_size + 0.1, 0, 1], [0.1, half_size, 1])\n",
    "        self.add_box([-half_size - 0.1, 0, 1], [0.1, half_size, 1])\n",
    "        self.add_box([0, half_size + 0.1, 1], [half_size, 0.1, 1])\n",
    "        self.add_box([0, -half_size - 0.1, 1], [half_size, 0.1, 1])\n",
    "\n",
    "        # internal attributes\n",
    "        self.goal_robot_id = None\n",
    "        self.path = None\n",
    "        self.approx_path = None\n",
    "        self.sg_pairs = None\n",
    "\n",
    "    def clear_obstacles(self):\n",
    "        self.occ_grid.fill(0)\n",
    "        self.obstacle_dict = {}\n",
    "        self.inflated_occ_grid = None\n",
    "\n",
    "    def random_obstacles(self, num_of_boxes = 8):\n",
    "        # add random obstacles with boxes.\n",
    "        # box_positions = [(-2.25, 2.25)]\n",
    "        box_positions = []\n",
    "\n",
    "        for _ in range(num_of_boxes):\n",
    "            x = random.randint(0, 4)\n",
    "            y = random.randint(0, 4)\n",
    "            x = x - 2\n",
    "            y = y - 2\n",
    "            box_positions.append((x, y))\n",
    "\n",
    "        # print(box_positions)\n",
    "        for box_pos in box_positions:\n",
    "            self.add_box([box_pos[0], box_pos[1], 0.5], [0.5, 0.5, 0.5])\n",
    "\n",
    "        self.obstacle_dict[\"box\"] = box_positions\n",
    "        self.get_inflated_occ_grid()\n",
    "\n",
    "    def add_box(self, box_pos, half_box_size):\n",
    "        # for occupancy grid, center is at upper left corner, unit is cm\n",
    "        half_size = MAZE_SIZE / 2.0\n",
    "        tmp = int(1 / OCC_GRID_RESOLUTION)\n",
    "        cx = (-box_pos[1] + half_size) * tmp\n",
    "        cy = (box_pos[0] + half_size) * tmp\n",
    "        x_size = half_box_size[1] * tmp\n",
    "        y_size = half_box_size[0] * tmp\n",
    "        for x in range(max(0, int(cx - x_size)), min(self.occ_grid_size, int(cx + x_size))):\n",
    "            for y in range(max(0, int(cy - y_size)), min(self.occ_grid_size, int(cy + y_size))):\n",
    "                self.occ_grid[x, y] = 1\n",
    "\n",
    "    def get_occupancy_grid(self):\n",
    "        return self.occ_grid\n",
    "\n",
    "    def get_small_occupancy_grid(self):\n",
    "        occ_grid_small = np.zeros((self.small_occ_grid_size, self.small_occ_grid_size), dtype=np.int8)\n",
    "        for i in range(self.small_occ_grid_size):\n",
    "            for j in range(self.small_occ_grid_size):\n",
    "                occ_grid_small[i, j] = (np.max(self.occ_grid[i*5:(i+1)*5, j*5:(j+1)*5]) == 1)\n",
    "        return occ_grid_small\n",
    "\n",
    "    def get_obstacle_dict(self):\n",
    "        return self.obstacle_dict.copy()\n",
    "\n",
    "    def load_obstacle_dict(self, obstacle_dict):\n",
    "        if \"box\" in obstacle_dict:\n",
    "            for box_pos in obstacle_dict[\"box\"]:\n",
    "                self.add_box([box_pos[0], box_pos[1], 0.5], [0.5, 0.5, 0.5])\n",
    "\n",
    "        self.obstacle_dict = obstacle_dict\n",
    "        \n",
    "    @classmethod\n",
    "    def load_small_occupancy_grid(cls, small_occ_grid):\n",
    "        maze = cls()\n",
    "        maze.occ_grid = np.zeros((maze.occ_grid_size, maze.occ_grid_size), dtype=np.uint8)\n",
    "        if len(small_occ_grid) != maze.small_occ_grid_size * maze.small_occ_grid_size:\n",
    "            raise RuntimeError(\"Input occupancy grid does not match hardcoded maze size!\")\n",
    "        \n",
    "        small_occ_grid = np.array(small_occ_grid).astype(np.int8).reshape(maze.small_occ_grid_size, -1)\n",
    "        for i in range(maze.small_occ_grid_size):\n",
    "            for j in range(maze.small_occ_grid_size):\n",
    "                if small_occ_grid[i, j] == 1:                    \n",
    "                    maze.occ_grid[i*5:(i+1)*5, j*5:(j+1)*5] = np.ones((5, 5), dtype=np.uint8)\n",
    "        return maze\n",
    "\n",
    "    def sample_start_goal(self):\n",
    "        while True:\n",
    "            start = [0] * self.robot.num_dim\n",
    "            goal = [0] * self.robot.num_dim\n",
    "            low_bounds = self.robot.get_joint_lower_bounds()\n",
    "            high_bounds = self.robot.get_joint_higher_bounds()\n",
    "            for i in range(self.robot.num_dim):\n",
    "                start[i] = random.uniform(low_bounds[i], high_bounds[i])\n",
    "                goal[i] = random.uniform(low_bounds[i], high_bounds[i])\n",
    "\n",
    "            if self.is_state_valid(start) and self.is_state_valid(goal):\n",
    "                self.start = start\n",
    "                self.goal = goal\n",
    "                break\n",
    "\n",
    "        print(\"Maze2d: start: {}\".format(self.start))\n",
    "        print(\"Maze2d: goal: {}\".format(self.goal))\n",
    "\n",
    "    def get_inflated_occ_grid(self):\n",
    "        if self.inflated_occ_grid is None:\n",
    "            tmp = np.zeros((self.occ_grid_size + 2, self.occ_grid_size + 2), dtype=np.uint8)\n",
    "            tmp[:self.occ_grid_size, :self.occ_grid_size] += self.occ_grid\n",
    "            tmp[1:self.occ_grid_size + 1, :self.occ_grid_size] += self.occ_grid\n",
    "            tmp[2:, :self.occ_grid_size] += self.occ_grid\n",
    "            tmp[:self.occ_grid_size, 1:self.occ_grid_size+1] += self.occ_grid\n",
    "            tmp[1:self.occ_grid_size + 1, 1:self.occ_grid_size+1] += self.occ_grid\n",
    "            tmp[2:, 1:self.occ_grid_size+1] += self.occ_grid\n",
    "            tmp[:self.occ_grid_size, 2:] += self.occ_grid\n",
    "            tmp[1:self.occ_grid_size + 1, 2:] += self.occ_grid\n",
    "            tmp[2:, 2:] += self.occ_grid\n",
    "            tmp[tmp > 0] = 1\n",
    "\n",
    "            self.inflated_occ_grid = tmp[1:self.occ_grid_size + 1, 1:self.occ_grid_size + 1]\n",
    "\n",
    "    def is_state_valid(self, robot_state):\n",
    "        # Inflate obstacle for collision checking\n",
    "        self.get_inflated_occ_grid()\n",
    "\n",
    "        y, x = robot_state[0], robot_state[1]\n",
    "        x = int((MAZE_SIZE / 2.0 - x) / 0.1)\n",
    "        y = int((y + MAZE_SIZE / 2.0) / 0.1)\n",
    "    \n",
    "        if 0 <= x and x < self.inflated_occ_grid.shape[0] and 0 <= y and y < self.inflated_occ_grid.shape[1]:\n",
    "            return (self.inflated_occ_grid[x, y] != 1)\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bb55b4",
   "metadata": {
    "id": "e7bb55b4"
   },
   "source": [
    "Let's generate a random environment and sample a random start and goal configuration of the robot and visualize the problem. The start configuration is shown in yellow and the goal configuration is shown in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274477a0",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1635870711191,
     "user": {
      "displayName": "Joel Loo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11818416161416341433"
     },
     "user_tz": -480
    },
    "id": "274477a0"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def visualize_data(occ_g, start_pos, goal_pos, path, predicted_path=None):\n",
    "    occ_g = np.array(occ_g).reshape(10, 10)\n",
    "    fig, ax = plt.subplots(1, figsize=(10, 10), dpi=100)\n",
    "    occ_grid_size = occ_g.shape[0]\n",
    "    tmp = occ_grid_size / 4.0 - 0.25\n",
    "    s = (10.0 / occ_grid_size * 100 / 2) ** 2 + 500\n",
    "    for i in range(occ_grid_size):\n",
    "        for j in range(occ_grid_size):\n",
    "            if occ_g[i,j] == 1:\n",
    "                plt.scatter(j/2.0 - tmp, tmp - i/2.0, color=\"black\", marker='s', s=s, alpha=1) # init\n",
    "\n",
    "    ax.add_patch(patches.Rectangle((start_pos[0]-0.1, start_pos[1]-0.1), 0.2, 0.2, facecolor='y'))\n",
    "    ax.add_patch(patches.Rectangle((goal_pos[0]-0.1, goal_pos[1]-0.1), 0.2, 0.2, facecolor='r'))\n",
    "    for i, next_pos in enumerate(path):\n",
    "        ax.text(next_pos[0]+0.06, next_pos[1]+0.06, str(i), {'color': 'g', 'size': 'large'})\n",
    "        ax.add_patch(patches.Rectangle((next_pos[0]-0.07, next_pos[1]-0.07), 0.14, 0.14, facecolor='g'))\n",
    "    if predicted_path is not None:\n",
    "        for i, predicted_pos in enumerate(predicted_path):\n",
    "            ax.text(predicted_pos[0] - 0.11, predicted_pos[1] - 0.11, str(i), {'color': 'b', 'size': 'large'})\n",
    "            ax.add_patch(patches.Rectangle((predicted_pos[0]-0.05, predicted_pos[1]-0.05), 0.1, 0.1, facecolor='b'))\n",
    "\n",
    "    ax.set_title(\"Visualization\")\n",
    "    ax.set_xlim(-2.5,2.5)\n",
    "    ax.set_ylim(-2.5,2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caeb304",
   "metadata": {
    "id": "2caeb304"
   },
   "outputs": [],
   "source": [
    "maze = Maze2D()\n",
    "maze.random_obstacles()\n",
    "maze.sample_start_goal()\n",
    "\n",
    "occ_grid = maze.get_small_occupancy_grid()\n",
    "visualize_data(occ_grid, maze.start, maze.goal, []) # Just visualise the environment for now, without any expert paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6442a3",
   "metadata": {
    "id": "eb6442a3"
   },
   "source": [
    "## Generate Data from Expert Path Planner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0fc1bb",
   "metadata": {
    "id": "1d0fc1bb"
   },
   "source": [
    "To train a neural path planner, we need to generate a database of path planned by an expert path planner. There are numerous choices of path planners we can use. In this particular case, since our robot state is continuous, we choose to use the classic PRM motion planner. It firstly samples valid configurations of robot uniformly in the whole space and attempts to connect those configurations if the path between the states are collision-free. It results in a dense roadmap that captures the connectivity of the space. Finally, a discrete motion planner such as A* is used to find a path between a given start and goal configurations in the space. Another reason that PRM is particularly useful here is that it is multi-query planner, meaning the generated roadmap can be used to solve multiple queries of different start and goal configurations. \n",
    "\n",
    "To generate our path dataset, we sample 200 different mazes with number of obstacles ranging from 5 to 14. In each environment, we sample 500 random configurations and attempt to connect all valid configurations. We save the resultant roadmap as a networkx graph. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6372553b",
   "metadata": {
    "id": "6372553b"
   },
   "source": [
    "### Generate Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d3e466",
   "metadata": {
    "id": "43d3e466"
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import itertools\n",
    "import random\n",
    "import utils\n",
    "\n",
    "\n",
    "def create_environments(root_dir='./dataset', env_num=0, sparse_num=100, dense_num=500):\n",
    "    if env_num == 0:\n",
    "        return\n",
    "    maze = Maze2D()\n",
    "    for i in range(env_num):\n",
    "        # save\n",
    "        directory = osp.join(root_dir, str(i))\n",
    "        if not osp.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "        num_of_boxes = 5 + i // 50\n",
    "\n",
    "        # env\n",
    "        maze.clear_obstacles()\n",
    "        maze.random_obstacles(num_of_boxes=num_of_boxes)\n",
    "        occ_grid = np.array(maze.get_occupancy_grid()).reshape(50, 50)\n",
    "        occ_grid_small = maze.get_small_occupancy_grid()\n",
    "        obstacle_dict = maze.get_obstacle_dict()\n",
    "        maze.sample_start_goal()\n",
    "        maze.robot.set_state(maze.start)\n",
    "\n",
    "        # dense states\n",
    "        states = []\n",
    "        col_status = []\n",
    "        low = maze.robot.get_joint_lower_bounds()\n",
    "        high = maze.robot.get_joint_higher_bounds()\n",
    "        for _ in range(dense_num):\n",
    "            random_state = [0] * maze.robot.num_dim\n",
    "            for i in range(maze.robot.num_dim):\n",
    "                random_state[i] = random.uniform(low[i], high[i])\n",
    "            col_status.append(maze.is_state_valid(random_state)) # mark collision states\n",
    "            states.append(random_state)\n",
    "\n",
    "        dense_G = nx.DiGraph()\n",
    "        dense_G.add_nodes_from([(\"n{}\".format(i), {\"coords\": ','.join(map(str, state)), \"col\": not col_status[i]}) for i, state in enumerate(states)])\n",
    "\n",
    "        # save\n",
    "        # node_pos = np.array(states)\n",
    "        node_pos = np.array([utils.state_to_numpy(dense_G.nodes[node]['coords']) for node in dense_G.nodes()])\n",
    "        utils.visualize_nodes(occ_grid_small, node_pos, None, None, show=False, save=True, file_name=osp.join(directory, \"dense.png\"))\n",
    "        node_pos = np.array([utils.state_to_numpy(dense_G.nodes[node]['coords']) for node in dense_G.nodes() if not dense_G.nodes[node]['col']])\n",
    "        utils.visualize_nodes(occ_grid_small, node_pos, None, None, show=False, save=True, file_name=osp.join(directory, \"dense_free.png\"))\n",
    "\n",
    "        print(\"connecting dense graph\")\n",
    "        nodes = dense_G.nodes()\n",
    "        node_pairs = itertools.combinations(nodes, 2)\n",
    "        # print(list(node_pairs))\n",
    "        for node_pair in node_pairs:\n",
    "            if not dense_G.has_edge(node_pair[0], node_pair[1]):\n",
    "                s1 = dense_G.nodes[node_pair[0]]['coords']\n",
    "                s2 = dense_G.nodes[node_pair[1]]['coords']\n",
    "                if utils.is_edge_free(maze, s1, s2):\n",
    "                    dense_G.add_edge(node_pair[0], node_pair[1])\n",
    "                    dense_G.add_edge(node_pair[1], node_pair[0])\n",
    "        for u,v in dense_G.edges:\n",
    "            dense_G[u][v]['weight'] = utils.calc_weight_states(dense_G.nodes[u]['coords'], dense_G.nodes[v]['coords'])\n",
    "\n",
    "        # save\n",
    "        nx.write_graphml(dense_G, osp.join(directory, \"dense_g.graphml\"))\n",
    "        with open(osp.join(directory, \"occ_grid.txt\"), 'w') as f:\n",
    "            np.savetxt(f, occ_grid_small.reshape(1, -1))\n",
    "        with open(osp.join(directory, \"obstacle_dict.json\"), 'w') as f:\n",
    "            json.dump(obstacle_dict, f)\n",
    "        utils.visualize_nodes(occ_grid_small, [], None, None, show=False, save=True, file_name=osp.join(directory, \"occ_grid.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbec7607",
   "metadata": {
    "id": "cbec7607"
   },
   "outputs": [],
   "source": [
    "create_environments(root_dir='./test_dataset', env_num=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d6af30",
   "metadata": {
    "id": "d5d6af30"
   },
   "source": [
    "### Generate Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c81a30c",
   "metadata": {
    "id": "1c81a30c"
   },
   "source": [
    "We want to collect multiple paths from a single generated environment. Therefore, we first load the saved roadmap graph, and subsequently nvoke Astar path planner to find shortest path between each possible pair of start and goal configurations. If a path is found, we add to our dataset.\n",
    "\n",
    "Here we ignore path that contains only 2 waypoints. Intuitively, a path with two waypoints means that the start configuration and goal configuration of the robot can be connected by a straigh line. As our environment is not very cluttered, the majority of the path in our dataset may constitute such a straight-line path. This imbalance might incur problems in the latter training stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67bf993",
   "metadata": {
    "id": "f67bf993"
   },
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import sys\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import utils\n",
    "\n",
    "import astar\n",
    "\n",
    "def generate_path(root_dir='./dataset', env_num=0, sample_num=500):\n",
    "    if env_num == 0:\n",
    "        return\n",
    "    print(f'Generating path for data in {root_dir}')\n",
    "    maze = Maze2D()\n",
    "\n",
    "    dataset = []\n",
    "    for i in range(env_num):\n",
    "        print(\"generating paths in env {}\".format(i))\n",
    "        maze.clear_obstacles()\n",
    "        \n",
    "        data_dir = osp.join(root_dir, str(i))\n",
    "        with open(osp.join(data_dir, \"obstacle_dict.json\"), 'r') as f:\n",
    "            obstacle_dict = json.load(f)\n",
    "            maze.load_obstacle_dict(obstacle_dict)\n",
    "\n",
    "        dense_G = nx.read_graphml(osp.join(data_dir, \"dense_g.graphml\"))\n",
    "        occ_grid = np.loadtxt(osp.join(data_dir, \"occ_grid.txt\")).tolist()\n",
    "\n",
    "        # sample trajectories\n",
    "        for start_n in dense_G.nodes():\n",
    "            if dense_G.nodes[start_n]['col']:\n",
    "                continue\n",
    "\n",
    "            for goal_n in dense_G.nodes():\n",
    "                if dense_G.nodes[goal_n]['col']:\n",
    "                    continue\n",
    "\n",
    "            goal_pos = utils.state_to_numpy(dense_G.nodes[goal_n]['coords']).tolist()\n",
    "            path_nodes, dis = astar.astar(dense_G, start_n, goal_n, occ_grid, None, None, None)\n",
    "\n",
    "            # sanity check\n",
    "            total_dist = 0\n",
    "            if len(path_nodes) > 2:\n",
    "                for i, node in enumerate(path_nodes):\n",
    "                    if i < len(path_nodes) - 1:\n",
    "                        start_pos = utils.state_to_numpy(dense_G.nodes[node]['coords']).tolist()\n",
    "                        next_pos = utils.state_to_numpy(dense_G.nodes[path_nodes[i + 1]]['coords']).tolist()\n",
    "                        dist = utils.calc_weight_states(start_pos, next_pos)\n",
    "                        total_dist += dist\n",
    "                # print(total_dist, dis)\n",
    "                assert np.allclose(total_dist, dis)\n",
    "\n",
    "            if len(path_nodes) > 2:\n",
    "                path = []\n",
    "                for i, node in enumerate(path_nodes):\n",
    "                    node_pos = utils.state_to_numpy(dense_G.nodes[node]['coords']).tolist()\n",
    "                    path.append(node_pos)\n",
    "\n",
    "                dataset.append([start_pos, goal_pos, occ_grid, path])\n",
    "    \n",
    "    json_path = osp.join(root_dir, 'data_path.json')\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c8acb3",
   "metadata": {
    "id": "a0c8acb3"
   },
   "outputs": [],
   "source": [
    "generate_path(root_dir=os.path.join(CURRENT_DIR, 'test_dataset/'), env_num=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ac7ba7",
   "metadata": {
    "id": "e9ac7ba7"
   },
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35749acd",
   "metadata": {
    "id": "35749acd"
   },
   "source": [
    "Our dataset contains a paths, a sequence of waypoints. However, our MLP and CNN models predicts only the next waypoints given the current robot configuration. Therefore, the dataset needs to be further processed so that a single training data gives the next waypoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6933062",
   "metadata": {
    "id": "d6933062"
   },
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "for root_dir in ['./dataset', './test_dataset']:\n",
    "    read_path = osp.join(root_dir, 'data_path.json')\n",
    "    with open(read_path, 'r') as _file:\n",
    "        data_path = json.load(_file)\n",
    "\n",
    "    dataset_waypoint = []\n",
    "    for data_point in data_path:\n",
    "        start_pos, goal_pos, occ_grid, path = data_point\n",
    "        for i in range(1, len(path)):\n",
    "            prev_pos = path[i - 1]\n",
    "            current_pos = path[i]\n",
    "            dataset_waypoint.append([prev_pos, goal_pos, occ_grid, current_pos])\n",
    "\n",
    "    save_path = osp.join(root_dir, 'data_waypoints.json')\n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(dataset_waypoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c44b554",
   "metadata": {
    "id": "1c44b554"
   },
   "source": [
    "# Visualization of the generated data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e90dc01",
   "metadata": {
    "id": "7e90dc01"
   },
   "source": [
    "Load the waypoint data and randomly visualize a single data point.\n",
    "\n",
    "- Black: obstacles\n",
    "- Red: goal position\n",
    "- Yellow: current position\n",
    "- Green: the next position the robot should take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502e2fc3",
   "metadata": {
    "id": "502e2fc3"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "WAYPOINT_DATA_FILE_PATH_TO_LOAD = os.path.join(CURRENT_DIR, 'dataset/data_waypoints.json')\n",
    "with open(WAYPOINT_DATA_FILE_PATH_TO_LOAD) as _file:\n",
    "    RAW_DATA = json.load(_file)\n",
    "\n",
    "idx = np.random.randint(len(RAW_DATA))\n",
    "current_pos, goal_pos, occ_grid, next_pos = RAW_DATA[idx]\n",
    "visualize_data(occ_grid, current_pos, goal_pos, [next_pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97037a0b",
   "metadata": {
    "id": "97037a0b"
   },
   "source": [
    "The baseline approach for this project is to test out different architectures on the problem of generating a sensible next waypoint given start and end goals. This can be iterated to get closer to the end goal. We also generate complete paths from the start to the end goal to serve as ground truth trajectories to benchmark our generated paths against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28881f30",
   "metadata": {
    "id": "28881f30"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "import os\n",
    "PATH_DATA_FILE_PATH_TO_LOAD = os.path.join(CURRENT_DIR, 'dataset/data_path.json')\n",
    "with open(PATH_DATA_FILE_PATH_TO_LOAD) as _file:\n",
    "    RAW_DATA = json.load(_file)\n",
    "\n",
    "idx = np.random.randint(len(RAW_DATA))\n",
    "current_pos, goal_pos, occ_grid, path = RAW_DATA[idx]\n",
    "visualize_data(occ_grid, current_pos, goal_pos, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a7a066",
   "metadata": {
    "id": "c0a7a066"
   },
   "source": [
    "# Proposed architectures\n",
    "\n",
    "We propose and evaluate the following architectures to solve the problem of predicting a good next waypoint in a path:\n",
    "* MLP. This is the most vanilla version of a neural network, and we try it to provide a baseline and determine the utility of learning to predict the next waypoint in a path without capitalizing on any structure in the problem.\n",
    "* CNN. In this variant, we attach a convolutional encoder to an MLP head. Since the input of an occupancy grid is a 2D image, we hope that the CNN can better inform the next waypoint prediction by extracting local features from the grid.\n",
    "\n",
    "Since the problem of planning a full path is one of generating a sequence of waypoints, we surmise that RNNs might be suitable representations for our sequential data. We explore the benefits of using the history of waypoints to predict the next waypoint with an RNN.\n",
    "\n",
    "The feedforward and recurrent architectures and their associated dataloaders are implemented in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7980230b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25222,
     "status": "ok",
     "timestamp": 1635870744229,
     "user": {
      "displayName": "Joel Loo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11818416161416341433"
     },
     "user_tz": -480
    },
    "id": "7980230b",
    "outputId": "7c067d22-9b93-4367-d969-4701e6c00020"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "\n",
    "# For deterministic execution\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fbda5a",
   "metadata": {
    "id": "21fbda5a"
   },
   "source": [
    "## Definitions of feedforward networks and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e301933",
   "metadata": {},
   "source": [
    "We implement a vanilla MLP that takes as input the vectorized occupancy grid concatenated in a single vector with the start and end goals. This MLP directly regresses the next waypoint position.\n",
    "\n",
    "![](img/mlp_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee5e1b7",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1635846498845,
     "user": {
      "displayName": "Joel Loo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11818416161416341433"
     },
     "user_tz": -480
    },
    "id": "cee5e1b7"
   },
   "outputs": [],
   "source": [
    "class MLPDataset(Dataset):\n",
    "    \"\"\"A dataset class for the MLP.\n",
    "    \n",
    "    Input: A vector that concat current position, goal position and occupancy grid vector\n",
    "    Output: Next position\n",
    "    \"\"\"\n",
    "    def __init__(self, raw_data, transform=None, target_transform=None, device=\"cpu\"):\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.device = device\n",
    "        self.dataset = raw_data\n",
    "        print(\"dataset size = {}\".format(len(self.dataset)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_pos, goal_pos, occ_grid, next_pos = self.dataset[idx]\n",
    "\n",
    "        dim = len(start_pos)\n",
    "        start_pos = torch.Tensor(start_pos)\n",
    "        goal_pos = torch.Tensor(goal_pos)\n",
    "        occ_grid = torch.Tensor(occ_grid)\n",
    "        next_pos = torch.Tensor(next_pos)\n",
    "\n",
    "        input = torch.cat((start_pos, goal_pos, occ_grid), dim=0).to(self.device)\n",
    "        next_pos = next_pos.to(self.device)\n",
    "\n",
    "        return input, next_pos\n",
    "    \n",
    "    def get_raw_data(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_from_file(cls, file_path, device='cpu'):\n",
    "        print(\"Loading data from {}\".format(file_path))\n",
    "        with open(file_path, 'r') as f:\n",
    "            dataset = json.load(f)\n",
    "        return cls(dataset, device=device)\n",
    "    \n",
    "    @classmethod\n",
    "    def format_input_for_inference(cls, input):\n",
    "        return input.view(1, -1)\n",
    "    \n",
    "    @classmethod\n",
    "    def format_input_for_motion_planning(cls, input, current_pos):\n",
    "        input[:2] = current_pos\n",
    "        return input.view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3cdbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel(nn.Module):\n",
    "    \"\"\"A trivially simple MLP model.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(104, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            #nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            #nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            #nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 64),\n",
    "            #nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.layers(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb5a1b7",
   "metadata": {},
   "source": [
    "As a next step from the vanilla MLP, we implement a CNN-based model which we hypothesize could better capture the spatial information in the occupancy grid and estimate better waypoints. Instead of vectorizing the occupancy grid, we pass it in as a 2D image to the convolutional encoder, which generates a 128-element feature vector. This is concatenated with the current position and end goal and passed into an MLP to regress the next waypoint.\n",
    "\n",
    "![](img/cnn_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mVAlfsa2fTZt",
   "metadata": {
    "executionInfo": {
     "elapsed": 413,
     "status": "ok",
     "timestamp": 1635871813598,
     "user": {
      "displayName": "Joel Loo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11818416161416341433"
     },
     "user_tz": -480
    },
    "id": "mVAlfsa2fTZt"
   },
   "outputs": [],
   "source": [
    "class MLPCNNDataset(Dataset):\n",
    "    def __init__(self, raw_data, transform=None, target_transform=None, device=\"cpu\"):\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.device = device\n",
    "        self.dataset = raw_data\n",
    "        print(\"dataset size = {}\".format(len(self.dataset)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_pos, goal_pos, occ_grid, next_pos = self.dataset[idx]\n",
    "\n",
    "        dim = len(start_pos)\n",
    "        start_pos = torch.Tensor(start_pos)\n",
    "        goal_pos = torch.Tensor(goal_pos)\n",
    "        occ_grid = torch.Tensor(occ_grid)\n",
    "        next_pos = torch.Tensor(next_pos)\n",
    "\n",
    "        positions = torch.cat((start_pos, goal_pos), dim=0).to(self.device)\n",
    "        occ_grid = torch.unsqueeze(torch.reshape(occ_grid, (10, 10)), dim=0).to(self.device)\n",
    "        next_pos = next_pos.to(self.device)\n",
    "\n",
    "        return (occ_grid, positions), next_pos\n",
    "    \n",
    "    def get_raw_data(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_from_file(cls, file_path, device='cpu'):\n",
    "        print(\"Loading data from {}\".format(file_path))\n",
    "        with open(file_path, 'r') as f:\n",
    "          dataset = json.load(f)\n",
    "        return cls(dataset, device=device)\n",
    "    \n",
    "    @classmethod\n",
    "    def format_input_for_inference(cls, input):\n",
    "        occ_grid, pos = input\n",
    "        return occ_grid.view(1, 1, 10, 10), pos.view(1, -1)\n",
    "    \n",
    "    @classmethod\n",
    "    def format_input_for_motion_planning(cls, input, current_pos):\n",
    "        occ_grid, pos = input\n",
    "        pos[:2] = current_pos\n",
    "        return occ_grid.view(1, 1, 10, 10), pos.view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6jZlJzfDHR",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1635846498847,
     "user": {
      "displayName": "Joel Loo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11818416161416341433"
     },
     "user_tz": -480
    },
    "id": "dd6jZlJzfDHR"
   },
   "outputs": [],
   "source": [
    "class MLPCNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPCNNModel, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 16*1, 4, stride=2, padding=1), # out = (16, 5, 5)\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(16, 32, 3, 2, 1), # out = (32, 3, 3)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 64, 3, 2, 1), # out = (64, 2, 2)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1), # out = (128, 1, 1)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128+4, 1024),\n",
    "            nn.BatchNorm1d(num_features=1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(num_features=1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 1024),\n",
    "            #nn.BatchNorm1d(num_features=1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 1024),\n",
    "            #nn.BatchNorm1d(num_features=1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 1024),\n",
    "            #nn.BatchNorm1d(num_features=1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 64),\n",
    "            #nn.BatchNorm1d(num_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        img, positions = inputs\n",
    "        img_features = self.conv(img)\n",
    "        features_positions = torch.cat((img_features, positions), dim=1)\n",
    "        output = self.fc(features_positions)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832dc3c0",
   "metadata": {
    "id": "832dc3c0"
   },
   "source": [
    "## Definitions of recurrent networks and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6dfa88",
   "metadata": {},
   "source": [
    "To try and better capture the sequential nature of the data, we implement an RNN trained on datasets comprising entire paths, instead of being trained on data that consists only of the ideal next waypoint given the current position (as we have done with MLP and CNN models). To avoid passing the occupancy grid into the RNN as input at each timestep, which could become prohibitive for large environments, we choose to encode the occupancy grid into the initial hidden state of the RNN. We employ an MLP we call the map embedding module, which outputs a 128-element vector that embeds the occupancy grid into the initial hidden state. Subsequently, at each timestep the input to the RNN is the concatenation of its current position at that timestep and the end goal. The predicted next waypoint becomes the current position for the subsequent timestep.\n",
    "\n",
    "![](img/rnn_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XfdqSWimQnU0",
   "metadata": {
    "id": "XfdqSWimQnU0"
   },
   "outputs": [],
   "source": [
    "class MLPRNNDataset(Dataset):\n",
    "    def __init__(self, raw_data, transform=None, target_transform=None, device=\"cpu\"):\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.device = device\n",
    "        self.dataset = raw_data\n",
    "        print(\"dataset size = {}\".format(len(self.dataset)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_pos, goal_pos, occ_grid, path = self.dataset[idx]\n",
    "        path_length = len(path) - 1\n",
    "        \n",
    "        start_seq = torch.Tensor(path[:-1]).to(self.device)\n",
    "        goal_pos = torch.Tensor(path[-1]).to(self.device)\n",
    "        input_seq = torch.cat((start_seq, goal_pos.repeat(path_length, 1)), dim=1)\n",
    "        label_seq = torch.Tensor(path[1:]).to(self.device)\n",
    "        occ_grid = torch.Tensor(occ_grid).to(self.device)\n",
    "\n",
    "        return (input_seq, occ_grid), label_seq\n",
    "    \n",
    "    def get_raw_data(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_from_file(cls, file_path, device='cpu'):\n",
    "        print(\"Loading data from {}\".format(file_path))\n",
    "        with open(file_path, 'r') as f:\n",
    "            dataset = json.load(f)\n",
    "        return cls(dataset, device=device)\n",
    "    \n",
    "    @classmethod\n",
    "    def format_input_for_inference(cls, input):\n",
    "        input_seq, occ_grid = input\n",
    "        return (\n",
    "            torch.unsqueeze(input_seq, 0),\n",
    "            occ_grid.view(1, -1)\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def format_input_for_motion_planning(cls, input, current_pos):\n",
    "        input_seq, occ_grid = input\n",
    "        goal = input_seq[0, 2:]\n",
    "        concat = torch.cat((current_pos, goal))\n",
    "        return concat.view(1, 1, -1), occ_grid.view(1, -1)\n",
    "\n",
    "import random\n",
    "import copy\n",
    "\n",
    "class MLPRNNDataloader():\n",
    "    def __init__(self, raw_data, batch_size, min_dataset_size=0, shuffle=True, \n",
    "                 drop_last=False, device=\"cpu\", uniform_path_length=None):\n",
    "        print(\"Raw data size: \", len(raw_data))\n",
    "        self.shuffle = shuffle\n",
    "            \n",
    "        if uniform_path_length is not None:\n",
    "            def truncate_fn(datum):\n",
    "                _, goal, occ_grid, path = datum\n",
    "                start = path[uniform_path_length - 1]\n",
    "                return (start, goal, occ_grid, path[:uniform_path_length+1])\n",
    "            \n",
    "            filtered_data = filter(lambda datum: len(datum[3]) - 1 >= uniform_path_length, raw_data)\n",
    "            truncated_data = list(map(truncate_fn, filtered_data))\n",
    "            self.datasets = {uniform_path_length:MLPRNNDataset(truncated_data, device=device)}\n",
    "            self.generators = {uniform_path_length:DataLoader(self.datasets[uniform_path_length], batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)}\n",
    "            self.path_lengths = [uniform_path_length]\n",
    "            print(\"All paths have length \", uniform_path_length)\n",
    "            print(\"Reduced dataset size: \", len(truncated_data))\n",
    "            return\n",
    "        \n",
    "        lengths = map(lambda datum: len(datum[3]) - 1, raw_data)\n",
    "        partitions = {k:list() for k in lengths}\n",
    "        for datum in raw_data:\n",
    "            _, _, _, path = datum\n",
    "            partitions[len(path) - 1].append(datum)\n",
    "\n",
    "        remove = []\n",
    "        for k, data_list in partitions.items():\n",
    "            if len(data_list) < min_dataset_size:\n",
    "                remove.append(k)\n",
    "                print(\"Removing all paths of length \", k, \": only \", len(data_list), \" samples\")\n",
    "            else:\n",
    "                print(\"Adding all paths of length \", k, \": \", len(data_list), \" samples\")\n",
    "        for k in remove:\n",
    "            del partitions[k]\n",
    "\n",
    "        self.datasets = {k:MLPRNNDataset(data_list, device=device) for k, data_list in partitions.items()}\n",
    "        self.generators = {k:DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last) for k, dataset in self.datasets.items()}\n",
    "        self.path_lengths = list(self.generators.keys())\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.tmp_path_lengths = copy.deepcopy(self.path_lengths)\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.tmp_path_lengths)\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if len(self.tmp_path_lengths) > 0:\n",
    "            next_path_length = copy.deepcopy(self.tmp_path_lengths[-1])\n",
    "            next_gen = self.generators[next_path_length]\n",
    "            self.tmp_path_lengths.pop()\n",
    "            return next_gen, next_path_length\n",
    "        else:\n",
    "            raise StopIteration\n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_from_file(cls, file_path, batch_size, train_frac, shuffle=True, drop_last=False, device=\"cpu\", uniform_path_length=None):\n",
    "      print(\"Loading data from {}\".format(file_path))\n",
    "      with open(file_path, 'r') as f:\n",
    "          dataset = json.load(f)\n",
    "\n",
    "      train_size = int(train_frac * len(dataset))\n",
    "      if train_size < len(dataset):\n",
    "          train_dataset = dataset[:train_size]\n",
    "          val_dataset = MLPRNNDataset(dataset[train_size:], device=device)\n",
    "      else:\n",
    "          train_dataset = dataset\n",
    "          val_dataset = MLPRNNDataset([], device=device)\n",
    "\n",
    "      return (\n",
    "          cls(train_dataset, batch_size, shuffle=shuffle, drop_last=drop_last, device=device, uniform_path_length=uniform_path_length), \n",
    "          DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b428581",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPRNNModel(nn.Module):\n",
    "    def __init__(self, latent_size):\n",
    "        super(MLPRNNModel, self).__init__()\n",
    "\n",
    "        self.map_embedding = nn.Sequential(\n",
    "            nn.Linear(100, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(512, latent_size)\n",
    "        )\n",
    "\n",
    "        self.goal_embedding =  nn.Sequential(\n",
    "            nn.Linear(4, latent_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(latent_size, latent_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(latent_size, latent_size),\n",
    "        )\n",
    "\n",
    "        self.rnn = nn.RNN(latent_size, latent_size, batch_first=True)\n",
    "\n",
    "        self.mlp_regression = nn.Sequential(\n",
    "            nn.Linear(latent_size, latent_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(latent_size, latent_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(latent_size, 2)\n",
    "        )\n",
    "        \n",
    "        self.latent_size = latent_size\n",
    "\n",
    "    def forward(self, inputs, h_init, initialise=False):\n",
    "        input_seq, occ_grid = inputs\n",
    "        batch_size, seq_length, num_dims = input_seq.shape\n",
    "\n",
    "        # h_init not explicitly used in this implementation; instead we overwrite h_init with the map embedding\n",
    "        if initialise:\n",
    "            h = torch.unsqueeze(self.map_embedding(occ_grid), 0) + h_init\n",
    "        else:\n",
    "            h = h_init\n",
    "\n",
    "        # RNN\n",
    "        g_seq = self.goal_embedding(input_seq)\n",
    "        h_seq, h_final = self.rnn(g_seq, h)\n",
    "        regressed_seq = self.mlp_regression(h_seq)\n",
    "\n",
    "        return regressed_seq, h_final\n",
    "    \n",
    "    def init_with_zeros(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.latent_size).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c60ebb",
   "metadata": {
    "id": "c8c60ebb"
   },
   "source": [
    "# Training\n",
    "\n",
    "We implement the training pass, and train and evaluate the architectures proposed in the previous section.\n",
    "\n",
    "## Implementing utilities\n",
    "\n",
    "Before implementing the training passes, we develop some utilities needed during training, to help visualise and evaluate the progress of training, and performance of the trained models.\n",
    "\n",
    "This is a utility to visualise the losses on the training and validation sets during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87df49be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "def init_training_loss_visualiser(num_epochs):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.set_xlim(0, num_epochs + 1)\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Training and validation loss')\n",
    "    \n",
    "    legend_lines = [Line2D([0], [0], color='b', lw=3),\n",
    "                    Line2D([0], [0], color='r', lw=3)]\n",
    "    ax.legend(legend_lines, ['Training loss', 'Validation loss'])\n",
    "    return fig, ax\n",
    "\n",
    "def visualise_loss(fig, ax, training_losses, val_losses):\n",
    "    epochs = list(range(len(training_losses)))\n",
    "    ax.plot(epochs, training_losses, 'b')\n",
    "    ax.plot(epochs, val_losses, 'r')\n",
    "    fig.canvas.draw()\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75936a4",
   "metadata": {
    "id": "d75936a4"
   },
   "source": [
    "This evaluates the average loss of the model on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9173314",
   "metadata": {
    "executionInfo": {
     "elapsed": 395,
     "status": "ok",
     "timestamp": 1635871772956,
     "user": {
      "displayName": "Joel Loo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11818416161416341433"
     },
     "user_tz": -480
    },
    "id": "f9173314"
   },
   "outputs": [],
   "source": [
    "def evaluate_test_metrics(test_dataloader, model, criterion, is_recurrent=False):\n",
    "    i = 0\n",
    "    total_loss = 0\n",
    "    test_dataloader = [test_dataloader] if not is_recurrent else test_dataloader\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for entry in test_dataloader:\n",
    "            if is_recurrent:\n",
    "                dataloader, path_length = entry\n",
    "                print(f\"Evaluating data with path length: {path_length}\")\n",
    "            else:\n",
    "                dataloader = entry\n",
    "            \n",
    "            for data in dataloader:\n",
    "                # Get batch of data\n",
    "                inputs, labels = data\n",
    "\n",
    "                # Check if an RNN is being evaluated, and initialise hidden state if so\n",
    "                if is_recurrent:\n",
    "                    batch_size = inputs[0].shape[0]\n",
    "                    h_init = model.init_with_zeros(batch_size)\n",
    "                    network_output, _ = model.forward(inputs, h_init, initialise=True)\n",
    "                else:\n",
    "                    network_output = model.forward(inputs)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = criterion(network_output, labels)\n",
    "                # Print statistics\n",
    "                total_loss += loss.detach().item()\n",
    "                i += 1\n",
    "        print(f'Average loss :{total_loss / i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8164a41e",
   "metadata": {
    "id": "8164a41e"
   },
   "source": [
    "This allows us to visualize the waypoints/paths generated by the model and compare it to ground truth waypoints/paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cf1131",
   "metadata": {
    "executionInfo": {
     "elapsed": 298,
     "status": "ok",
     "timestamp": 1635871782874,
     "user": {
      "displayName": "Joel Loo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11818416161416341433"
     },
     "user_tz": -480
    },
    "id": "f7cf1131"
   },
   "outputs": [],
   "source": [
    "def visualize_model_output(dataset, model, is_recurrent=False):\n",
    "    idx = np.random.randint(len(dataset))\n",
    "    inputs, _ = dataset[idx]\n",
    "    current_pos, goal_pos, occ_grid, gt = dataset.get_raw_data(idx)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if is_recurrent:\n",
    "            h_init = model.init_with_zeros(1)\n",
    "            predicted, _ = model.forward(dataset.format_input_for_inference(inputs), h_init, initialise=True)\n",
    "            predicted_path = torch.squeeze(predicted.detach()).cpu().numpy()\n",
    "            visualize_data(occ_grid, gt[0], gt[-1], gt[1:], predicted_path=predicted_path)\n",
    "        else:\n",
    "            predicted = model.forward(dataset.format_input_for_inference(inputs))\n",
    "            visualize_data(occ_grid, current_pos, goal_pos, [gt], predicted_path=[predicted.detach().cpu().numpy().flatten()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ba7532",
   "metadata": {},
   "source": [
    "## Implementing training pass\n",
    "\n",
    "Implement the training procedures for both feedforward as well as recurrent network architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d72ea0d",
   "metadata": {},
   "source": [
    "We implement a training loop for feedforward architectures designed to regress the waypoints given the occupancy grid, start and end goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2ca8b3",
   "metadata": {
    "executionInfo": {
     "elapsed": 306,
     "status": "ok",
     "timestamp": 1635871777962,
     "user": {
      "displayName": "Joel Loo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11818416161416341433"
     },
     "user_tz": -480
    },
    "id": "8e2ca8b3"
   },
   "outputs": [],
   "source": [
    "def train_feedforward(\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        eval_dataloader,\n",
    "        criterion=None,\n",
    "        batch_size=64,\n",
    "        learning_rate=1e-4,\n",
    "        num_epochs=10,\n",
    "        model_string=\"checkpoint\"\n",
    "        ):\n",
    "    if criterion is None:\n",
    "        print(f\"Using MSE loss as default loss function\")\n",
    "        criterion = torch.nn.MSELoss()\n",
    "    \n",
    "    model_save_path = os.path.join(CURRENT_DIR, f'models/{model_string}.pt')\n",
    "    print(f\"Checkpoint save path: {model_save_path}\")\n",
    "\n",
    "    fig, ax = init_training_loss_visualiser(num_epochs)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "    # Run the training loop\n",
    "    i = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        print(f\"Epoch {epoch} starts\")\n",
    "        total_loss = 0\n",
    "        print(\"--------Training\")\n",
    "        for data in train_dataloader:\n",
    "            # Get batch of data\n",
    "            inputs, labels = data\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Perform forward pass\n",
    "            #print(inputs.shape, labels.shape)\n",
    "            network_output = model.forward(inputs)\n",
    "            #print(network_output.shape)\n",
    "            # Compute loss\n",
    "            loss = criterion(network_output, labels)\n",
    "            # Ensure no funny numerics\n",
    "            assert not torch.isnan(loss).any()\n",
    "            # Perform backward pass\n",
    "            loss.backward()\n",
    "            # \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 10.0)\n",
    "            # Perform optimization\n",
    "            optimizer.step()\n",
    "            # Print statistics\n",
    "            total_loss += loss.detach().item()\n",
    "            i += 1\n",
    "            if i % 100 == 0:\n",
    "                average_loss = total_loss / i\n",
    "                rms = np.sqrt(average_loss)\n",
    "                print('-----------------Average loss/RMS error after mini-batch %5d, epoch %d : %.3f, %.3f' % (i, epoch, average_loss, rms))\n",
    "        train_losses.append(total_loss / i)\n",
    "\n",
    "        # Evaluate on validation set at end of each epoch\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "\n",
    "        for data in eval_dataloader:\n",
    "            # Get batch of data\n",
    "            inputs, labels = data\n",
    "            # Perform forward pass\n",
    "            network_output = model.forward(inputs)\n",
    "            # Compute loss\n",
    "            loss = criterion(network_output, labels)\n",
    "            total_loss += loss.detach().item()\n",
    "\n",
    "        average_loss = total_loss / len(eval_dataloader)\n",
    "        rms = np.sqrt(average_loss)\n",
    "        print(\"--------Evaluation\")\n",
    "        print('-----------------Total loss/RMS error after epoch %5d: %.3f, %.3f' % (epoch, average_loss, rms))\n",
    "        \n",
    "        val_losses.append(average_loss)\n",
    "        visualise_loss(fig, ax, train_losses, val_losses)\n",
    "            \n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "    \n",
    "    print(f\"-----------------saved epoch {epoch} to \", model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7103ce93",
   "metadata": {},
   "source": [
    "We implement a training loop to train recurrent architectures that can retain memory of past waypoints while regressing future waypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc0e86c",
   "metadata": {
    "id": "4bc0e86c"
   },
   "outputs": [],
   "source": [
    "def normalize_gradient(net):\n",
    "    grad_norm_sq=0\n",
    "    for p in net.parameters():\n",
    "        grad_norm_sq += p.grad.data.norm()**2\n",
    "\n",
    "    grad_norm=math.sqrt(grad_norm_sq)\n",
    "    if grad_norm<1e-4:\n",
    "        net.zero_grad()\n",
    "        print('grad norm close to zero')\n",
    "    else:    \n",
    "        for p in net.parameters():\n",
    "             p.grad.data.div_(grad_norm)\n",
    "\n",
    "    return grad_norm\n",
    "\n",
    "def train_recurrent(\n",
    "        model,\n",
    "        network_hidden_size,\n",
    "        train_dataloaders,\n",
    "        eval_dataloader,\n",
    "        criterion=None,\n",
    "        batch_size=64,\n",
    "        learning_rate=1e-4,\n",
    "        num_epochs=10,\n",
    "        model_string=\"checkpoint\"\n",
    "        ):\n",
    "    if criterion is None:\n",
    "        print(f\"Using MSE loss as default loss function\")\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        \n",
    "    fig, ax = init_training_loss_visualiser(num_epochs)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "        \n",
    "    model_save_path = os.path.join(CURRENT_DIR, f'models/{model_string}.pt')\n",
    "    print(f\"Checkpoint save path: {model_save_path}\")\n",
    "        \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "    # Run the training loop\n",
    "    i = 0\n",
    "    frac = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        print(f\"Epoch {epoch} starts\")\n",
    "        total_loss = 0\n",
    "        print(\"--------Training\")\n",
    "\n",
    "        for dataloader, path_length in train_dataloaders:\n",
    "            print(\"Training on dataset with path length: \", path_length)\n",
    "            for data in dataloader:\n",
    "                # Get batch of data\n",
    "                inputs, labels = data\n",
    "                # Zero the gradients\n",
    "                optimizer.zero_grad()\n",
    "                # Initialise the hidden state\n",
    "                h_init = torch.zeros(1, inputs[0].shape[0], network_hidden_size).to(DEVICE)\n",
    "                # Perform forward pass\n",
    "                output, _ = model.forward(inputs, h_init, initialise=True)\n",
    "                # Compute loss\n",
    "                loss = criterion(output, labels)\n",
    "                # Ensure no funny numerics\n",
    "                assert not torch.isnan(loss).any()\n",
    "                # Perform backward pass\n",
    "                loss.backward()\n",
    "                # Clip gradients to prevent explosion\n",
    "                normalize_gradient(model)\n",
    "                # Perform optimization\n",
    "                optimizer.step()\n",
    "\n",
    "                # Print statistics\n",
    "                total_loss += loss.detach().item()\n",
    "                i += 1\n",
    "                frac += float(inputs[0].shape[0]) / float(batch_size)\n",
    "                if i % 100 == 0:\n",
    "                    #average_loss = total_loss / frac\n",
    "                    average_loss = total_loss / i\n",
    "                    rms = np.sqrt(average_loss)\n",
    "                    print('-----------------Average loss/RMS error after mini-batch %5d, epoch %d : %.3f, %.3f' % (i, epoch, average_loss, rms))\n",
    "                    \n",
    "        train_losses.append(total_loss / i)\n",
    "        \n",
    "        # Evaluate on validation set at end of each epoch\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "\n",
    "        for data in eval_dataloader:\n",
    "            # Get batch of data\n",
    "            inputs, labels = data\n",
    "            # Perform forward pass\n",
    "            h_init = torch.zeros(1, inputs[0].shape[0], network_hidden_size).to(DEVICE)\n",
    "            network_output, _ = model.forward(inputs, h_init, initialise=True)\n",
    "            # Compute loss\n",
    "            loss = criterion(network_output, labels)\n",
    "            total_loss += loss.detach().item()\n",
    "\n",
    "        average_loss = total_loss / len(eval_dataloader)\n",
    "        rms = np.sqrt(average_loss)\n",
    "        print(\"--------Evaluation\")\n",
    "        print('-----------------Total loss/RMS error after epoch %5d: %.3f, %.3f' % (epoch, average_loss, rms))\n",
    "        \n",
    "        val_losses.append(average_loss)\n",
    "        visualise_loss(fig, ax, train_losses, val_losses)\n",
    "                    \n",
    "        # save at each epoch\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'rnn_hidden_size': model.latent_size\n",
    "        }, model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d572a7e",
   "metadata": {
    "id": "5d572a7e"
   },
   "source": [
    "## Training feedforward architectures\n",
    "\n",
    "Implement some utilities for loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d181de7d",
   "metadata": {
    "executionInfo": {
     "elapsed": 392,
     "status": "ok",
     "timestamp": 1635871785033,
     "user": {
      "displayName": "Joel Loo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11818416161416341433"
     },
     "user_tz": -480
    },
    "id": "d181de7d"
   },
   "outputs": [],
   "source": [
    "def init_feedforward_dataloaders(dataset_cls, train_frac, batch_size):\n",
    "    train_val_dataset = dataset_cls.load_dataset_from_file(os.path.join(CURRENT_DIR, 'dataset/data_waypoints.json'), device=DEVICE)\n",
    "    train_size = int(len(train_val_dataset) * train_frac)\n",
    "    \n",
    "    train_set, val_set = torch.utils.data.random_split(train_val_dataset, [train_size, len(train_val_dataset) - train_size])\n",
    "    #train_set = torch.utils.data.Subset(train_val_dataset, np.arange(train_size))\n",
    "    #val_set = torch.utils.data.Subset(train_val_dataset, np.arange(train_size, len(train_val_dataset)))\n",
    "    test_set = dataset_cls.load_dataset_from_file(os.path.join(CURRENT_DIR, 'test_dataset/data_waypoints.json'), device=DEVICE)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    eval_dataloader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_dataloader, eval_dataloader, test_dataloader, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc11256",
   "metadata": {
    "id": "acc11256"
   },
   "source": [
    "Train a vanilla MLP to predict next waypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47ed1f4",
   "metadata": {
    "id": "e47ed1f4"
   },
   "outputs": [],
   "source": [
    "# Parameters specific to this instantiation of the model and its training\n",
    "model_string = \"MLPfinal_data2\"\n",
    "train_frac = 0.9 # Proportion of dataset to use as train set (otherwise will be validation)\n",
    "batch_size = 128\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 8\n",
    "criterion = torch.nn.MSELoss()\n",
    "model_cls = MLPModel\n",
    "train_loader, eval_loader, test_loader, test_raw = init_feedforward_dataloaders(MLPDataset, train_frac, batch_size)\n",
    "\n",
    "# Train\n",
    "model = model_cls().to(DEVICE)\n",
    "train_feedforward(model, train_loader, eval_loader, criterion=criterion, batch_size=batch_size, \n",
    "                  learning_rate=learning_rate, num_epochs=num_epochs, model_string=model_string)\n",
    "\n",
    "# Evaluate\n",
    "evaluate_test_metrics(test_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39c23b5",
   "metadata": {
    "id": "a39c23b5"
   },
   "outputs": [],
   "source": [
    "# Visualize random prediction on test set\n",
    "visualize_model_output(test_raw, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63406d01",
   "metadata": {
    "id": "63406d01"
   },
   "source": [
    "Train a vanilla CNN with a fully-connected head to predict next waypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a52a3f",
   "metadata": {
    "id": "b9a52a3f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parameters specific to this instantiation of the model and its training\n",
    "model_string = \"MLPCNNfinal_data2\"\n",
    "train_frac = 0.9 # Proportion of dataset to use as train set (otherwise will be validation)\n",
    "batch_size = 128\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 8\n",
    "criterion = torch.nn.MSELoss()\n",
    "model_cls = MLPCNNModel\n",
    "train_loader, eval_loader, test_loader, test_raw = init_feedforward_dataloaders(MLPCNNDataset, train_frac, batch_size)\n",
    "\n",
    "# Train\n",
    "model = model_cls().to(DEVICE)\n",
    "train_feedforward(model, train_loader, eval_loader, criterion=criterion, batch_size=batch_size, \n",
    "                  learning_rate=learning_rate, num_epochs=num_epochs, model_string=model_string)\n",
    "\n",
    "# Evaluate\n",
    "evaluate_test_metrics(test_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc8b342",
   "metadata": {
    "id": "2bc8b342"
   },
   "outputs": [],
   "source": [
    "# Visualize random prediction on test set\n",
    "visualize_model_output(test_raw, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3165f2df",
   "metadata": {
    "id": "3165f2df"
   },
   "source": [
    "## Training recurrent architectures\n",
    "\n",
    "Implement utilities for loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053f9b94",
   "metadata": {
    "id": "053f9b94"
   },
   "outputs": [],
   "source": [
    "def init_recurrent_dataloaders(cls, train_frac, batch_size):\n",
    "    train_loaders, data_loader = MLPRNNDataloader.load_dataset_from_file(\n",
    "        os.path.join(CURRENT_DIR, 'dataset/data_path.json'), batch_size, train_frac,\n",
    "        shuffle=True, drop_last=True, device=DEVICE\n",
    "    )\n",
    "    \n",
    "    TEST_DATA_PATH = os.path.join(CURRENT_DIR, 'test_dataset/data_path.json')\n",
    "    test_loader, _ = MLPRNNDataloader.load_dataset_from_file(TEST_DATA_PATH, batch_size, 1.0, \n",
    "                                                             shuffle=False, drop_last=False, device=DEVICE)\n",
    "    test_set = cls.load_dataset_from_file(TEST_DATA_PATH, device=DEVICE)\n",
    "    \n",
    "    return train_loaders, data_loader, test_loader, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2f66be",
   "metadata": {
    "id": "9d2f66be"
   },
   "source": [
    "Implement a basic RNN to predict the path from start to goal. At each timestep, the input to the RNN is the concatenation of its current position and the goal position, and the output is the next waypoint it should head towards. The RNN is conditioned on the obstacle grid - this is done by encoding the obstacle grid in a feature vector using an MLP, and adding the feature vector to the RNN's initial hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48936469",
   "metadata": {
    "id": "48936469",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parameters specific to this instantiation of the model and its training\n",
    "model_string = \"MLPRNNfinal7_data2\"\n",
    "train_frac = 0.95 # Proportion of dataset to use as train set (otherwise will be validation)\n",
    "batch_size = 64\n",
    "learning_rate = 1e-6\n",
    "num_epochs = 8\n",
    "criterion = torch.nn.MSELoss()\n",
    "model_cls = MLPRNNModel\n",
    "network_hidden_size = 128\n",
    "train_loaders, eval_loader, test_loader, test_raw = init_recurrent_dataloaders(MLPRNNDataset, train_frac, batch_size)\n",
    "\n",
    "# Train\n",
    "model = model_cls(network_hidden_size).to(DEVICE)\n",
    "train_recurrent(model, network_hidden_size, train_loaders, eval_loader, criterion=criterion, batch_size=batch_size, \n",
    "                learning_rate=learning_rate, num_epochs=num_epochs, model_string=model_string)\n",
    "\n",
    "# Evaluate\n",
    "evaluate_test_metrics(test_loader, model, criterion, is_recurrent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92e079b",
   "metadata": {
    "id": "c92e079b"
   },
   "outputs": [],
   "source": [
    "# Visualize random prediction on test set\n",
    "visualize_model_output(test_raw, model, is_recurrent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f9875d",
   "metadata": {
    "id": "71f9875d"
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "We present here the consolidated results from training and evaluating the proposed network architectures. We also focus on running evaluating the performance of our models on a full motion planning test set-up. Our previous evaluation focused on predicting waypoints one step ahead. In contrast we will now run the inference iteratively to continuously generate waypoints based on the last generated waypoint, and attempt to generate a full path linking the start to the end goal.\n",
    "\n",
    "## Evaluating performance on training/validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c956aef8",
   "metadata": {
    "id": "c956aef8"
   },
   "source": [
    "As shown above, we trained both the MLP and CNN models on our training dataset for 8 epochs. In the figure below, we can see the losses on both training and validation flattening out after 8 epochs. The losses converge similarly for both.\n",
    "\n",
    "<table><tr>\n",
    "    <td> <img src=\"img/mlp_training1.png\" alt=\"MLP\" width=\"400\"/> </td>\n",
    "    <td> <img src=\"img/cnn_training1.png\" alt=\"CNN\" width=\"400\"/> </td>\n",
    "</tr></table>\n",
    "<p style=\"text-align: center;\"><b>Fig. 1:</b> Losses for MLP (left) and CNN (right)</p>\n",
    "\n",
    "While we were able to consistently get the RNN to converge in training loss, we observed significant fluctuations in the validation losses. The RNN would also overfit easily as the validation loss would sometimes start to increase within 1-2 epochs of training. We tuned the hyperparameters of the training to try to achieve better convergence of validation loss and generalization, by using a lower learning rate of 1e-6, and training for 6 epochs only. The losses for the RNN are shown in the figure below.\n",
    "<img src=\"img/rnn_training.png\" alt=\"RNN\" width=\"400\"/>\n",
    "<p style=\"text-align: center;\"><b>Fig. 2:</b> Losses for RNN</p>\n",
    "\n",
    "As observed, the validation loss started to rise continuously after 6 epochs of training, before the training loss had properly converged. To avoid overfitting, we chose to use the model trained for up to 6 epochs.\n",
    "\n",
    "We present below the final losses achieved on both the training, validation and test sets. We present both MSE (mean-squared error) directly taken from the loss, as well as its square root, the RMS (root-mean-squared) error. These loss values intuitively give us a sense of how far away each waypoints predicted by our model is from the ground truth waypoint generated by our expert planner. In particular, the RMS error is an indication of this distance in Euclidean space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5065de99",
   "metadata": {},
   "source": [
    "|                                       \t| MLP \t|      CNN      \t| RNN \t|\n",
    "|---------------------------------------\t|:---:\t|:-------------:\t|:---:\t|\n",
    "| Final training loss (MSE/RMS error)       | 0.01 / 0.1    \t| 0.01 / 0.1 \t|  0.05 / 0.224  \t|\n",
    "| Final validation loss (MSE/RMS error)     | 0.063 / 0.251    \t|  0.064 / 0.253  \t|  0.403 / 0.635   \t|\n",
    "| Final test loss (MSE/RMS error)           | 0.184 / 0.429      | 0.227 / 0.476     | 0.327 / 0.572      |\n",
    "<p style=\"text-align: center;\"><b>Table 1:</b> Losses for the various network models </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7de464",
   "metadata": {
    "id": "ea7de464"
   },
   "source": [
    "## Evaluating performance on motion planning task\n",
    "\n",
    "We are also interested in evaluating how our models will perform on the full motion planning task. To do so, we created a motion planning test dataset of 100 environments. In each environment we have sampled start and goal positions, and computed a feasible path between them using A* search.\n",
    "\n",
    "In this setup, we will input the occupancy grid and end goals to our models. We will then iteratively generate a next waypoint based on our current position, beginning with the start goal. We provide several metrics to assess the performance of our models on the motion planning task.\n",
    "\n",
    "Based on preliminary visualizations of our models' output, we have observed that it appears to be challenging for our models to learn to always avoid obstacles with our dataset. Waypoints can sometimes be generated inside obstacles, although more often the straight-line path to the next generated waypoint will clip an obstacle. This is similar to observations in [2], which use additional subroutines to check and clean up the output of their neural networks for plan generation.\n",
    "\n",
    "In the light of this, we will evaluate the performance of the motion planning test based on the following criteria:\n",
    "* Hard success rate at generating feasible path to goal. This is defined as the network being able to generate a sequence of waypoints reaching the goal with a given tolerance of 0.2m within 15 steps while not colliding with obstacles along the way. Equivalently, it is $\\frac{1}{N}\\sum\\limits_{i=1}^{N}S_{i}C_{i}$, where $S_i, C_i$ are indicator variables for reaching the goal, and colliding with obstacles respectively.\n",
    "* Soft success rate at generating feasible path to goal. We replace $C_i$ in the hard success rate with a soft measure of collisions, given by the proportion of the generated path that is in freespace. Equivalently, it is \n",
    "$\\frac{1}{N} \\sum\\limits_{i=1}^{N}S_i\\big(1 - \\frac{p_{collision}}{p_i}\\big)$, where $p_i$ is the length of the predicted path and $p_{collision}$ is the length of the predicted path that overlaps with an obstacle.\n",
    "* Path length ratio, which captures the length of the predicted path compared to ground truth path length. This is a measure of how quickly or efficiently the a path generated by the network can take us to the goal, as compared with the path generated by our expert planner.  Equivalently, it is  $\\frac{l_i}{\\max(p_i, l_i)}$ where $l_i, p_i$ are the shortest path length and network predicted path length respectively. $l_i$ will be approximated using the length of the ground truth path computed by our expert planner.\n",
    "\n",
    "The path length ratio and hard success rate are both elements of the success-weighted path length (SPL) metric from [1], which we consider separately here to analyze the performance of our network in finer detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f4c823",
   "metadata": {},
   "source": [
    "First, we implement the utilities for running motion planning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c0fd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def single_motion_planning_test(model, dataset, idx, max_iters=10, is_recurrent=False, check_collision=False):\n",
    "    status = 'SUCCESS'\n",
    "    inputs, _ = dataset[idx]\n",
    "    _, goal_pos, occ_grid, gt = dataset.get_raw_data(idx)\n",
    "    maze = Maze2D.load_small_occupancy_grid(occ_grid)\n",
    "    \n",
    "    current_pos = np.array(gt[0], dtype=np.float32)\n",
    "    goal_pos = np.array(goal_pos)\n",
    "    it = 0\n",
    "    predicted_path = [gt[0]]\n",
    "    planned_path = [gt[0]]\n",
    "    hidden_state = None if not is_recurrent else model.init_with_zeros(1)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_col_length = 0\n",
    "        total_path_length = 0\n",
    "        while np.linalg.norm(current_pos - goal_pos) > 0.5:\n",
    "            if it >= max_iters:\n",
    "                status = 'NOT-REACH'\n",
    "                break\n",
    "\n",
    "            formatted = dataset.format_input_for_motion_planning(inputs, torch.from_numpy(current_pos).to(DEVICE))\n",
    "            if is_recurrent:\n",
    "                output, hidden_state = model.forward(formatted, hidden_state, initialise=(it==0))\n",
    "                predicted = torch.squeeze(output.detach()).cpu().numpy().flatten()\n",
    "            else:\n",
    "                output = model.forward(formatted)\n",
    "                predicted = output.detach().cpu().numpy().flatten()\n",
    "            \n",
    "            predicted_path.append(predicted.tolist())\n",
    "            current_pos = predicted\n",
    "            \n",
    "            if check_collision:\n",
    "                col_length = utils.get_edge_collision_length(maze, predicted_path[-2], predicted_path[-1])\n",
    "                total_col_length += col_length\n",
    "                edge_length = utils.cal_edge_length(predicted_path[-2], predicted_path[-1])\n",
    "                total_path_length += edge_length\n",
    "                if col_length > 0.000001:\n",
    "                    status = 'COLLIDED'\n",
    "\n",
    "            it += 1\n",
    "        if check_collision:\n",
    "            col_prop = total_col_length / total_path_length\n",
    "        else:\n",
    "            col_prop = 0\n",
    "        return status, np.array(predicted_path), occ_grid, gt, col_prop\n",
    "    \n",
    "def test_over_data(model, data, dataset_cls, number=100, is_recurrent=False, check_collision=False):\n",
    "    hard_success_count = 0\n",
    "    soft_success_count = 0.0\n",
    "    data = copy.deepcopy(data)\n",
    "    random.shuffle(data)\n",
    "    count = 0\n",
    "    test_set = dataset_cls(data, device=DEVICE)\n",
    "    path_length_ratio = 0.0\n",
    "    number = len(data) if number < 0 else number\n",
    "    \n",
    "    for idx in range(number):\n",
    "        if count % 10 == 0:\n",
    "          print(\"Tested \", count)\n",
    "        count += 1\n",
    "        status, predicted_path, _, gt_path, col_prop = single_motion_planning_test(\n",
    "            model, test_set, idx, is_recurrent=is_recurrent, check_collision=check_collision)\n",
    "        if status == 'SUCCESS' or status == 'COLLIDED':\n",
    "            hard_success_count += int(status == 'SUCCESS')\n",
    "            soft_success_count += (1 - col_prop)\n",
    "            predicted_path = np.array(predicted_path)\n",
    "            gt_path = np.array(gt_path)\n",
    "            predicted_path_length = np.sum(np.linalg.norm(predicted_path[1:,:] - predicted_path[:-1,:], axis=1))\n",
    "            gt_path_length = np.sum(np.linalg.norm(gt_path[1:,:] - gt_path[:-1,:], axis=1))\n",
    "            path_length_ratio += gt_path_length / max(predicted_path_length, gt_path_length)\n",
    "            \n",
    "    return (\n",
    "        float(hard_success_count) / number, \n",
    "        soft_success_count / number, \n",
    "        0 if soft_success_count == 0 else path_length_ratio / soft_success_count\n",
    "    )\n",
    "\n",
    "def load_feedforward_model(model_cls, model_string):\n",
    "    model = model_cls()\n",
    "    model_dir = os.path.join(CURRENT_DIR, 'models')\n",
    "    model.load_state_dict(torch.load(os.path.join(model_dir, model_string + \".pt\")))\n",
    "    model.to(DEVICE)\n",
    "    return model\n",
    "\n",
    "def load_recurrent_model(model_cls, model_string):\n",
    "    model_dir = os.path.join(CURRENT_DIR, 'models')\n",
    "    model_dict = torch.load(os.path.join(model_dir, model_string + \".pt\"))\n",
    "    model_hidden_size = model_dict['rnn_hidden_size']\n",
    "    model = model_cls(model_hidden_size)\n",
    "    model.load_state_dict(model_dict['model_state_dict'])\n",
    "    model.to(DEVICE)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4c530f",
   "metadata": {
    "id": "1d4c530f"
   },
   "source": [
    "### Visualising individual motion planning tests\n",
    "\n",
    "We visualise the planned paths iteratively generated by our proposed network architectures on randomly selected test cases from our test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8b251e",
   "metadata": {},
   "source": [
    "1. Visualize paths generated from the MLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd83595",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = MLPDataset.load_dataset_from_file(os.path.join(CURRENT_DIR, 'test_dataset/data_path.json'), device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b36b13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a random test case, perform motion planning with iterative inferences and visualise results\n",
    "model = load_feedforward_model(MLPModel, \"MLP\")\n",
    "idx = np.random.randint(len(test_set))\n",
    "status, predicted_path, occ_grid, gt_path, _ = single_motion_planning_test(\n",
    "    model, test_set, idx, check_collision=True)\n",
    "print(f'Status: {status}')\n",
    "visualize_data(occ_grid, gt_path[0], gt_path[-1], gt_path[1:], predicted_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c8b97d",
   "metadata": {},
   "source": [
    "2. Visualize paths generated from the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66lhQFLQpsmJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 717,
     "status": "ok",
     "timestamp": 1635873493080,
     "user": {
      "displayName": "Joel Loo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11818416161416341433"
     },
     "user_tz": -480
    },
    "id": "66lhQFLQpsmJ",
    "outputId": "2fd84f6f-844b-4de8-9cc7-4edcee6c489f"
   },
   "outputs": [],
   "source": [
    "test_set = MLPCNNDataset.load_dataset_from_file(os.path.join(CURRENT_DIR, 'test_dataset/data_path.json'), device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dH-s-Ojp0ps",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 872
    },
    "executionInfo": {
     "elapsed": 1033,
     "status": "ok",
     "timestamp": 1635873611259,
     "user": {
      "displayName": "Joel Loo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11818416161416341433"
     },
     "user_tz": -480
    },
    "id": "7dH-s-Ojp0ps",
    "outputId": "32f67b16-c24f-4d0f-806b-7c59ba5adac7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select a random test case, perform motion planning with iterative inferences and visualise results\n",
    "model = load_feedforward_model(MLPCNNModel, \"CNN\")\n",
    "idx = np.random.randint(len(test_set))\n",
    "status, predicted_path, occ_grid, gt_path, _ = single_motion_planning_test(\n",
    "    model, test_set, idx, check_collision=False)\n",
    "print(f'Status: {status}')\n",
    "visualize_data(occ_grid, gt_path[0], gt_path[-1], gt_path[1:], predicted_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4fde5a",
   "metadata": {},
   "source": [
    "3. Visualize paths generated from the RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab9f983",
   "metadata": {
    "id": "6ab9f983"
   },
   "outputs": [],
   "source": [
    "test_set = MLPRNNDataset.load_dataset_from_file(os.path.join(CURRENT_DIR, 'test_dataset/data_path.json'), device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24f4872",
   "metadata": {
    "id": "a24f4872",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Select a random test case, perform motion planning with iterative inferences and visualise results\n",
    "model = load_recurrent_model(MLPRNNModel, \"RNN\")\n",
    "idx = np.random.randint(len(test_set))\n",
    "status, predicted_path, occ_grid, gt_path, _ = single_motion_planning_test(\n",
    "    model, test_set, idx, is_recurrent=True, check_collision=True, max_iters=10)\n",
    "print(f'Status: {status}')\n",
    "visualize_data(occ_grid, gt_path[0], gt_path[-1], gt_path[1:], predicted_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41195536",
   "metadata": {
    "id": "41195536"
   },
   "source": [
    "### Overall performance on test dataset\n",
    "\n",
    "We evaluate the performance of our networks at motion planning, over the entire test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc5f19a",
   "metadata": {
    "id": "6dc5f19a"
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(CURRENT_DIR, 'test_dataset/data_path.json')) as _file:\n",
    "    data = json.load(_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb73654",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Motion planning test with MLP ###\n",
    "# Provide the model-specific parameters\n",
    "model_cls = MLPModel\n",
    "dataset_cls = MLPDataset\n",
    "\n",
    "model = load_feedforward_model(MLPModel, \"MLP\")\n",
    "hard_success_rate, soft_success_rate, path_length_ratio = test_over_data(\n",
    "    model, data, dataset_cls, number=-1, is_recurrent=False, check_collision=True)\n",
    "print(\"Hard success rate: \", hard_success_rate, \" Soft success rate: \", soft_success_rate,\n",
    "      \" Path length ratio: \", path_length_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e50e83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Motion planning test with MLP ###\n",
    "# Provide the model-specific parameters\n",
    "model_cls = MLPCNNModel\n",
    "dataset_cls = MLPCNNDataset\n",
    "\n",
    "model = load_feedforward_model(MLPCNNModel, \"CNN\")\n",
    "hard_success_rate, soft_success_rate, path_length_ratio = test_over_data(\n",
    "    model, data, dataset_cls, number=-1, is_recurrent=False, check_collision=True)\n",
    "print(\"Hard success rate: \", hard_success_rate, \" Soft success rate: \", soft_success_rate,\n",
    "      \" Path length ratio: \", path_length_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38016011",
   "metadata": {
    "id": "38016011"
   },
   "outputs": [],
   "source": [
    "### Motion planning test with RNN ###\n",
    "# Provide the model-specific parameters\n",
    "model_cls = MLPRNNModel\n",
    "dataset_cls = MLPRNNDataset\n",
    "\n",
    "# Evaluate performance on test set\n",
    "model = load_recurrent_model(MLPRNNModel, \"RNN\")\n",
    "hard_success_rate, soft_success_rate, path_length_ratio = test_over_data(\n",
    "    model, data, dataset_cls, number=-1, is_recurrent=True, check_collision=True)\n",
    "print(\"Hard success rate: \", hard_success_rate, \" Soft success rate: \", soft_success_rate,\n",
    "      \" Path length ratio: \", path_length_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8469cf5c",
   "metadata": {},
   "source": [
    "We summarize the results of the motion planning test into the table below.\n",
    "\n",
    "|                                       \t| MLP \t|      CNN      \t| RNN \t|\n",
    "|------------------------\t|:---:\t|:-------------:\t|:---:\t|\n",
    "| Hard success rate (%)     | 21.2  \t| 22.8 \t|   6.3  \t|\n",
    "| Soft success rate (%)     | 91.0    \t|  90.4  \t| 63.0     \t|\n",
    "| Path length ratio         | 1.10     | 1.10     |  1.59     |\n",
    "<p align=\"text-align: center;\"><b>Table 2:</b> Results of motion planning test</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3d2929",
   "metadata": {
    "id": "bc3d2929"
   },
   "source": [
    "# Discussion of results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87dca85",
   "metadata": {
    "id": "b87dca85"
   },
   "source": [
    "From our results we made a few key observations:\n",
    "* MLP and CNN models perform similarly\n",
    "* RNN model is much harder to train (does not generalize as easily as MLP and CNN do), and on our training data scores lower overall in terms of success rate than the MLP and CNN\n",
    "* However, the RNN has a higher path length ratio score compared to the MLP and CNN\n",
    "* Our hard success rate scores across all models remain relatively low, with up to 22.8% for the CNN\n",
    "\n",
    "We will discuss the performance of the MLP and CNN models first, and follow that with a discussion of the RNN's performance. We will explore the observations and possible reasons for them in the following discussions.\n",
    "\n",
    "### Discussing performance of MLP and CNN models\n",
    "We expected that the CNN model would outperform the MLP by a larger margin, since we anticipated the addition of a convolutional encoder to help aggregate features of the environment locally, which could aid in planning. However, the results show that both networks are about on par, with the CNN doing slightly better in terms of hard success rate. We attribute this to two factors. \n",
    "1. Our occupancy grid is a compressed and relatively low-dimensional representation (of size 10x10) of the environment, and so the CNN's ability to find local structure in high-dimensional images is rendered less useful by our input format. If our occupancy grid input were less compressed and much higher-dimensional (or even 3D), the CNN might then provide a significant boost in performance.\n",
    "\n",
    "2. The planning problem may not be solved well using only features extracted from local structure. To get from any arbitrary point A to point B efficiently, we will likely need features at a global scale (for us to find a coarse path) and features at a local scale (to avoid obstacles efficiently and minimize travel time). Thus, the planning problem might be more easily solved using all features generated at each scale of the occupancy grid, rather than just a single feature vector generated from repeated downsampling and pooling in a CNN.\n",
    "\n",
    "Aside from this, we observe that the MLP and CNN models do learn to generate paths quite well. We show some successful examples of planning with the MLP and CNN models below.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"img/mlp_planning_3.png\" width=\"300\"/></td>\n",
    "        <td> <img src=\"img/mlp_planning2.png\" width=\"300\"/></td>\n",
    "        <td> <img src=\"img/mlp_planning1.png\" width=\"300\"/></td>\n",
    "        <td> <img src=\"img/mlp_planning4.png\" width=\"300\"/></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> <img src=\"img/cnn_planning3.png\" width=\"300\"/></td>\n",
    "        <td> <img src=\"img/cnn_planning2.png\" width=\"300\"/></td>\n",
    "        <td> <img src=\"img/cnn_planning1.png\" width=\"300\"/></td>\n",
    "        <td> <img src=\"img/cnn_planning4.png\" width=\"300\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "<p style=\"text-align: center;\"><b>Fig. 3:</b> Planning results for MLP (top row) and CNN (bottom row) on our test set. <b>Yellow</b> is the start goal, <b>red</b> is the end goal, <b>green</b> points are ground truth waypoints from our expert planner and <b>blue</b> points are predicted waypoints from our neural networks (including the start goal). Black boxes are obstacles. </p>\n",
    "\n",
    "The MLP and CNN models are able to generalize to unseen environments and start and end goals, and have learned to set waypoints to avoid obstacles. This is clearly shown through some of the examples which require the robot to take a more complicated curving path to reach the end goal. The MLP and CNN are clearly able to generate such curving paths to avoid obstacles and reach the end goal safely.\n",
    "\n",
    "As noted before, we observe that similar to [2] and [3], it is challenging to ensure that neural networks plan paths that precisely always avoid obstacles. In [2] and [3], the authors use the neural networks as a data-driven way to efficiently sample good possible waypoints and combine the output of the neural networks with a classical planner algorithm similar to Rapidly-exploring Randomized Trees (RRT) that checks the output for collisions. Our visualizations show that our networks will occasionally generate waypoints in obstacles. We attribute to low hard success rate across all our models to this.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"img/mlp_clipping1.png\" width=\"500\"/></td>\n",
    "        <td> <img src=\"img/cnn_clipping1.png\" width=\"500\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "<p style=\"text-align: center;\"><b>Fig. 4:</b> Clipping of obstacles and planning into obstacles with MLP and CNN</p>\n",
    "\n",
    "Based on our soft success rate metric, we see that both the MLP and CNN generate waypoints such that the straight line connecting these waypoints lies within an obstacle 10% of the time. As with [2] and [3], this issue can be mitigated by combining our networks with a classical planner that does rigourous collision-checking.\n",
    "\n",
    "### Discussing performance of RNN model\n",
    "The RNN model was harder to train than the MLP or CNN models and more hyperparameter tuning was required for the training to lower validation loss. Our RNN also ended up scoring lower than both MLP and CNN models in terms of hard and soft success rates. We believe that these phenomena can be attributed to the structure of our problem. As we formulated it, each iteration of the motion planning task has the Markov property, i.e. that the next waypoint generated only has to depend on the current waypoint, and not the entire history of prior waypoints.\n",
    "\n",
    "Contrary to our initial hypothesis, providing information on the entire sequence of waypoints may have made it harder to learn to predict the next waypoint instead of easier. The RNN would have to learn a much more complex mapping from the entire history of waypoints to the next waypoint. This mapping may not be easy to learn, especially since minor changes in the obstacle configuration or start/end goals can result in drastically different paths being taken and hence drastically different waypoint sequences.\n",
    "\n",
    "Nevertheless, the RNN has still learned to generate paths and can generalize to some extent to our test set.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"img/rnn_planning1.png\" width=\"500\"/></td>\n",
    "        <td> <img src=\"img/rnn_planning2.png\" width=\"500\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "<p style=\"text-align: center;\"><b>Fig. 5:</b> Planning results for RNN</p>\n",
    "\n",
    "As with the MLP and CNN, the RNN also shows issues with clipping the obstacles. This problem is more prevalent, as shown by its significantly lower soft success rate of 63%, indicating that its paths clip obstacles 37% of the time. \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"img/rnn_clipping1.png\" width=\"500\"/></td>\n",
    "        <td> <img src=\"img/rnn_clipping2.png\" width=\"500\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "<p style=\"text-align: center;\"><b>Fig. 6:</b> Clipping of obstacles and planning into obstacles with RNN</p>\n",
    "\n",
    "As can be seen, the clipping issue is worse for the RNN, and waypoints can sometimes be generated inside obstacles. We again attribute this to the difficulty in learning the family of sequences that can take us from the start to the end goal.\n",
    "\n",
    "Even when doubling the size of the dataset for the RNN, we observed that the hard and soft success rates did not change significantly for the RNN. This suggests to us that the RNN is having issues with the structure of our data, namely that the function mapping sequences of waypoints to the next waypoint to take is too complex. We experimented with larger RNN hidden layer sizes, though that did not improve the scores significantly and made overfitting worse as well.\n",
    "\n",
    "We observed that one of the ways the RNN fails to generate good paths seems to be because it does not capture high frequency components in the path, and hence does not capture bends very well. It seems to have learned to predict paths along simple arcs or (approximately) straight lines that lead to it generating paths through obstacles instead of avoiding them. We believe the higher path length ratio achieved by the RNN is not because of its ability to predict more efficient paths than the expert planner, but because it tends to go along straight lines or arcs while disregarding obstacles.\n",
    "\n",
    "To better learn higher-frequency information and more complex paths, using an embedding of the sequence like the positional encoding in [4] might perhaps aid its ability to learn more variation in its generated paths.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"img/rnn_lowfreq1.png\" width=\"500\"/></td>\n",
    "        <td> <img src=\"img/rnn_lowfreq2.png\" width=\"500\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "<p style=\"text-align: center;\"><b>Fig. 7:</b> Inability to capture high frequency components in the path</p>\n",
    "\n",
    "\n",
    "### Summary\n",
    "Based on both our metrics and visualizations, the MLP and CNN are able to perform fairly well at predicting a useful next waypoint. Although they are not precise in avoiding obstacles, as evidenced by the low hard success rate of ~20%, their output is still useful to intelligently sample waypoints that can take us closer to the goal while avoiding obstacles. In fact, their path length ratios are both above 1, indicating that the paths that they have taken are slightly more efficient than the expert planner's sampled path.\n",
    "\n",
    "The RNN model also does learn to predict useful next waypoints, although its success rate is significantly lower. It also tends to plan into obstacles, although that is a difficulty that can be mitigated by combining it with a classical planner. We surmise that with stronger regularisation, the RNN might be better able to learn to predict good waypoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec02905",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c44653",
   "metadata": {},
   "source": [
    "[1] P. Anderson, A. Chang, D.S. Chaplot,A.  Dosovitskiy, S. Gupta, V. Koltun, J. Kosecka, J. Malik, R. Mottaghi, M. Savva, A.R. Zamir. On Evaluation of Embodied Navigation Agents. arXiv preprint 1807.06757, 2018\n",
    "\n",
    "[2] A.H. Qureshi, A. Simeonov, M.J. Bency, M.C. Yip. Motion Planning Networks. ICRA, 2019.\n",
    "\n",
    "[3] A. H. Qureshi , Y.L. Miao, A. Simeonov and M.C. Yip. Motion Planning Networks: Bridging the Gap Between Learning-Based and Classical Motion Planners. IEEE Transactions on Robotics 37 (2021): 48-66.\n",
    "\n",
    "[4] M. Tancik, P.P. Srinivasan, B. Mildenhall, S. Fridovich-Keil, N. Raghavan, U. Singhal, R. Ramamoorthi, J.T. Barron, R. Ng. Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains. NeurIPS, 2020."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "3hN-efIwnVxD",
    "f313fd07",
    "aeaa7faa",
    "1c44b554",
    "c0a7a066",
    "c8c60ebb",
    "bc3d2929"
   ],
   "name": "our-notebook.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "b1513172bc75592431e687543e7a7e71518769f9e336f9ec823ad71ba40cd915"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
